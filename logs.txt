* 
* ==> Audit <==
* |--------------|------------------------------|----------|------|---------|---------------------|---------------------|
|   Command    |             Args             | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|------------------------------|----------|------|---------|---------------------|---------------------|
| start        |                              | minikube | ilia | v1.32.0 | 19 Apr 24 12:52 MSK | 19 Apr 24 12:54 MSK |
| addons       |                              | minikube | ilia | v1.32.0 | 19 Apr 24 12:55 MSK | 19 Apr 24 12:55 MSK |
| addons       | list                         | minikube | ilia | v1.32.0 | 19 Apr 24 12:55 MSK | 19 Apr 24 12:55 MSK |
| addons       | ingress enable               | minikube | ilia | v1.32.0 | 19 Apr 24 12:55 MSK | 19 Apr 24 12:55 MSK |
| addons       | enable ingress               | minikube | ilia | v1.32.0 | 19 Apr 24 12:55 MSK | 19 Apr 24 12:56 MSK |
| ip           |                              | minikube | ilia | v1.32.0 | 19 Apr 24 12:56 MSK | 19 Apr 24 12:56 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 19 Apr 24 15:52 MSK | 19 Apr 24 15:53 MSK |
| ip           |                              | minikube | ilia | v1.32.0 | 19 Apr 24 15:53 MSK | 19 Apr 24 15:53 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 19 Apr 24 16:00 MSK | 19 Apr 24 16:02 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 22 Apr 24 12:11 MSK | 22 Apr 24 12:11 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 22 Apr 24 12:32 MSK | 22 Apr 24 12:32 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 22 Apr 24 13:11 MSK | 22 Apr 24 13:11 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 22 Apr 24 13:11 MSK | 22 Apr 24 13:11 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 23 Apr 24 01:42 MSK | 23 Apr 24 01:42 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 23 Apr 24 11:23 MSK | 23 Apr 24 11:23 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 25 Apr 24 03:03 MSK |                     |
| update-check |                              | minikube | ilia | v1.32.0 | 26 Apr 24 10:07 MSK | 26 Apr 24 10:07 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 03 May 24 09:48 MSK | 03 May 24 09:48 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 06 May 24 09:31 MSK | 06 May 24 09:32 MSK |
| addons       | enable metrics-server        | minikube | ilia | v1.32.0 | 06 May 24 09:34 MSK | 06 May 24 09:34 MSK |
| addons       | list                         | minikube | ilia | v1.32.0 | 06 May 24 09:34 MSK | 06 May 24 09:34 MSK |
| addons       | enable dashboard             | minikube | ilia | v1.32.0 | 06 May 24 09:34 MSK | 06 May 24 09:34 MSK |
| addons       | list                         | minikube | ilia | v1.32.0 | 06 May 24 09:34 MSK | 06 May 24 09:34 MSK |
| addons       | enable default-storageclass  | minikube | ilia | v1.32.0 | 06 May 24 09:35 MSK | 06 May 24 09:35 MSK |
| addons       | enable metrics-server        | minikube | ilia | v1.32.0 | 06 May 24 09:35 MSK | 06 May 24 09:35 MSK |
| addons       | enable storage-provisioner   | minikube | ilia | v1.32.0 | 06 May 24 09:35 MSK | 06 May 24 09:35 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 06 May 24 09:50 MSK | 06 May 24 09:50 MSK |
| service      | prometheus-server-ext        | minikube | ilia | v1.32.0 | 06 May 24 10:02 MSK | 06 May 24 10:02 MSK |
| service      | grafana-ext                  | minikube | ilia | v1.32.0 | 06 May 24 10:03 MSK | 06 May 24 10:03 MSK |
| service      | grafana-ext                  | minikube | ilia | v1.32.0 | 06 May 24 10:08 MSK |                     |
| start        |                              | minikube | ilia | v1.32.0 | 06 May 24 10:08 MSK | 06 May 24 10:09 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 06 May 24 10:11 MSK | 06 May 24 10:11 MSK |
| service      | grafana-ext                  | minikube | ilia | v1.32.0 | 06 May 24 10:11 MSK | 06 May 24 10:11 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 07 May 24 11:34 MSK | 07 May 24 11:35 MSK |
| service      | grafana-ext                  | minikube | ilia | v1.32.0 | 07 May 24 11:36 MSK | 07 May 24 11:36 MSK |
| update-check |                              | minikube | ilia | v1.32.0 | 08 May 24 10:12 MSK | 08 May 24 10:12 MSK |
| start        |                              | minikube | ilia | v1.32.0 | 08 May 24 10:21 MSK | 08 May 24 10:21 MSK |
| service      | service/task31-nginx-service | minikube | ilia | v1.32.0 | 08 May 24 10:31 MSK |                     |
| service      | task31-nginx-service         | minikube | ilia | v1.32.0 | 08 May 24 10:31 MSK |                     |
| service      | task31-nginx-service         | minikube | ilia | v1.32.0 | 08 May 24 10:32 MSK |                     |
|--------------|------------------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/05/08 10:21:03
Running on machine: ilia
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0508 10:21:03.359497   16101 out.go:296] Setting OutFile to fd 1 ...
I0508 10:21:03.359582   16101 out.go:348] isatty.IsTerminal(1) = true
I0508 10:21:03.359586   16101 out.go:309] Setting ErrFile to fd 2...
I0508 10:21:03.359593   16101 out.go:348] isatty.IsTerminal(2) = true
I0508 10:21:03.359753   16101 root.go:338] Updating PATH: /home/ilia/.minikube/bin
W0508 10:21:03.359818   16101 root.go:314] Error reading config file at /home/ilia/.minikube/config/config.json: open /home/ilia/.minikube/config/config.json: no such file or directory
I0508 10:21:03.360076   16101 out.go:303] Setting JSON to false
I0508 10:21:03.370799   16101 start.go:128] hostinfo: {"hostname":"ilia","uptime":1482,"bootTime":1715151382,"procs":321,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"kali-rolling","kernelVersion":"6.6.9-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"5bb99d49-f66d-49da-96cf-93d08dde2171"}
I0508 10:21:03.370853   16101 start.go:138] virtualization: kvm host
I0508 10:21:03.373098   16101 out.go:177] üòÑ  minikube v1.32.0 –Ω–∞ Debian kali-rolling
I0508 10:21:03.375666   16101 notify.go:220] Checking for updates...
I0508 10:21:03.376029   16101 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0508 10:21:03.376649   16101 driver.go:378] Setting default libvirt URI to qemu:///system
I0508 10:21:03.400743   16101 docker.go:122] docker version: linux-20.10.25+dfsg1:
I0508 10:21:03.400988   16101 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0508 10:21:03.425767   16101 info.go:266] docker info: {ID:IXK3:3YOP:24UG:7VW6:6VM4:7KOY:5N7L:M4AV:OGZB:NZXH:DHVO:YWML Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:33 SystemTime:2024-05-08 10:21:03.41832038 +0300 MSK LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.9-amd64 OperatingSystem:Kali GNU/Linux Rolling OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16110280704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ilia Labels:[] ExperimentalBuild:false ServerVersion:20.10.25+dfsg1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1.6.24~ds1-1 Expected:1.6.24~ds1-1} RuncCommit:{ID:1.1.12+ds1-2 Expected:1.1.12+ds1-2} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0508 10:21:03.425821   16101 docker.go:295] overlay module found
I0508 10:21:03.426712   16101 out.go:177] ‚ú®  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä–∞–π–≤–µ—Ä docker –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
I0508 10:21:03.429778   16101 start.go:298] selected driver: docker
I0508 10:21:03.429790   16101 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ilia:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0508 10:21:03.429909   16101 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0508 10:21:03.430041   16101 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0508 10:21:03.453722   16101 info.go:266] docker info: {ID:IXK3:3YOP:24UG:7VW6:6VM4:7KOY:5N7L:M4AV:OGZB:NZXH:DHVO:YWML Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:false NGoroutines:33 SystemTime:2024-05-08 10:21:03.447251915 +0300 MSK LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.6.9-amd64 OperatingSystem:Kali GNU/Linux Rolling OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16110280704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ilia Labels:[] ExperimentalBuild:false ServerVersion:20.10.25+dfsg1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1.6.24~ds1-1 Expected:1.6.24~ds1-1} RuncCommit:{ID:1.1.12+ds1-2 Expected:1.1.12+ds1-2} InitCommit:{ID: Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0508 10:21:03.454578   16101 cni.go:84] Creating CNI manager for ""
I0508 10:21:03.454589   16101 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 10:21:03.454597   16101 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ilia:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0508 10:21:03.455946   16101 out.go:177] üëç  –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è control plane —É–∑–µ–ª minikube –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ minikube
I0508 10:21:03.457470   16101 cache.go:121] Beginning downloading kic base image for docker with docker
I0508 10:21:03.458651   16101 out.go:177] üöú  –°–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –±–∞–∑–æ–≤—ã–π –æ–±—Ä–∞–∑ ...
I0508 10:21:03.460346   16101 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0508 10:21:03.460500   16101 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0508 10:21:03.460569   16101 preload.go:148] Found local preload: /home/ilia/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0508 10:21:03.460580   16101 cache.go:56] Caching tarball of preloaded images
I0508 10:21:03.460768   16101 preload.go:174] Found /home/ilia/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0508 10:21:03.460785   16101 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0508 10:21:03.460936   16101 profile.go:148] Saving config to /home/ilia/.minikube/profiles/minikube/config.json ...
I0508 10:21:03.481223   16101 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0508 10:21:03.481257   16101 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0508 10:21:03.481281   16101 cache.go:194] Successfully downloaded all kic artifacts
I0508 10:21:03.481324   16101 start.go:365] acquiring machines lock for minikube: {Name:mk16a4eab4558e26e7f4d04c3659b85c53a117fe Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0508 10:21:03.481446   16101 start.go:369] acquired machines lock for "minikube" in 73.123¬µs
I0508 10:21:03.481461   16101 start.go:96] Skipping create...Using existing machine configuration
I0508 10:21:03.481469   16101 fix.go:54] fixHost starting: 
I0508 10:21:03.481633   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:03.502841   16101 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0508 10:21:03.502884   16101 fix.go:128] unexpected machine state, will restart: <nil>
I0508 10:21:03.505989   16101 out.go:177] üîÑ  –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π docker container –¥–ª—è "minikube" ...
I0508 10:21:03.507648   16101 cli_runner.go:164] Run: docker start minikube
I0508 10:21:03.951634   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:03.970647   16101 kic.go:430] container "minikube" state is running.
I0508 10:21:03.970939   16101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 10:21:03.989208   16101 profile.go:148] Saving config to /home/ilia/.minikube/profiles/minikube/config.json ...
I0508 10:21:03.989443   16101 machine.go:88] provisioning docker machine ...
I0508 10:21:03.989481   16101 ubuntu.go:169] provisioning hostname "minikube"
I0508 10:21:03.989522   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:04.009084   16101 main.go:141] libmachine: Using SSH client type: native
I0508 10:21:04.009786   16101 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0508 10:21:04.009802   16101 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0508 10:21:04.010415   16101 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:37530->127.0.0.1:32772: read: connection reset by peer
I0508 10:21:07.181531   16101 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0508 10:21:07.181608   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:07.201788   16101 main.go:141] libmachine: Using SSH client type: native
I0508 10:21:07.202262   16101 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0508 10:21:07.202284   16101 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0508 10:21:07.328520   16101 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0508 10:21:07.328578   16101 ubuntu.go:175] set auth options {CertDir:/home/ilia/.minikube CaCertPath:/home/ilia/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ilia/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ilia/.minikube/machines/server.pem ServerKeyPath:/home/ilia/.minikube/machines/server-key.pem ClientKeyPath:/home/ilia/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ilia/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ilia/.minikube}
I0508 10:21:07.328611   16101 ubuntu.go:177] setting up certificates
I0508 10:21:07.328677   16101 provision.go:83] configureAuth start
I0508 10:21:07.328773   16101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 10:21:07.369452   16101 provision.go:138] copyHostCerts
I0508 10:21:07.371595   16101 exec_runner.go:144] found /home/ilia/.minikube/ca.pem, removing ...
I0508 10:21:07.371617   16101 exec_runner.go:203] rm: /home/ilia/.minikube/ca.pem
I0508 10:21:07.371698   16101 exec_runner.go:151] cp: /home/ilia/.minikube/certs/ca.pem --> /home/ilia/.minikube/ca.pem (1074 bytes)
I0508 10:21:07.371997   16101 exec_runner.go:144] found /home/ilia/.minikube/cert.pem, removing ...
I0508 10:21:07.372006   16101 exec_runner.go:203] rm: /home/ilia/.minikube/cert.pem
I0508 10:21:07.372064   16101 exec_runner.go:151] cp: /home/ilia/.minikube/certs/cert.pem --> /home/ilia/.minikube/cert.pem (1115 bytes)
I0508 10:21:07.372367   16101 exec_runner.go:144] found /home/ilia/.minikube/key.pem, removing ...
I0508 10:21:07.372377   16101 exec_runner.go:203] rm: /home/ilia/.minikube/key.pem
I0508 10:21:07.372434   16101 exec_runner.go:151] cp: /home/ilia/.minikube/certs/key.pem --> /home/ilia/.minikube/key.pem (1675 bytes)
I0508 10:21:07.372694   16101 provision.go:112] generating server cert: /home/ilia/.minikube/machines/server.pem ca-key=/home/ilia/.minikube/certs/ca.pem private-key=/home/ilia/.minikube/certs/ca-key.pem org=ilia.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0508 10:21:07.631639   16101 provision.go:172] copyRemoteCerts
I0508 10:21:07.631697   16101 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0508 10:21:07.631753   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:07.664109   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:07.763820   16101 ssh_runner.go:362] scp /home/ilia/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0508 10:21:07.816549   16101 ssh_runner.go:362] scp /home/ilia/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0508 10:21:07.853339   16101 ssh_runner.go:362] scp /home/ilia/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0508 10:21:07.895105   16101 provision.go:86] duration metric: configureAuth took 566.40609ms
I0508 10:21:07.895127   16101 ubuntu.go:193] setting minikube options for container-runtime
I0508 10:21:07.895394   16101 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0508 10:21:07.895466   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:07.929969   16101 main.go:141] libmachine: Using SSH client type: native
I0508 10:21:07.930634   16101 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0508 10:21:07.930654   16101 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0508 10:21:08.081127   16101 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0508 10:21:08.081138   16101 ubuntu.go:71] root file system type: overlay
I0508 10:21:08.081224   16101 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0508 10:21:08.081282   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.103088   16101 main.go:141] libmachine: Using SSH client type: native
I0508 10:21:08.103648   16101 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0508 10:21:08.103791   16101 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0508 10:21:08.248644   16101 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0508 10:21:08.248757   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.271124   16101 main.go:141] libmachine: Using SSH client type: native
I0508 10:21:08.271438   16101 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0508 10:21:08.271453   16101 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0508 10:21:08.392638   16101 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0508 10:21:08.392656   16101 machine.go:91] provisioned docker machine in 4.403200853s
I0508 10:21:08.392667   16101 start.go:300] post-start starting for "minikube" (driver="docker")
I0508 10:21:08.392682   16101 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0508 10:21:08.392752   16101 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0508 10:21:08.392815   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.414257   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:08.509723   16101 ssh_runner.go:195] Run: cat /etc/os-release
I0508 10:21:08.513587   16101 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0508 10:21:08.513641   16101 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0508 10:21:08.513664   16101 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0508 10:21:08.513678   16101 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0508 10:21:08.513700   16101 filesync.go:126] Scanning /home/ilia/.minikube/addons for local assets ...
I0508 10:21:08.514022   16101 filesync.go:126] Scanning /home/ilia/.minikube/files for local assets ...
I0508 10:21:08.514166   16101 start.go:303] post-start completed in 121.485848ms
I0508 10:21:08.514237   16101 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0508 10:21:08.514316   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.536917   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:08.619792   16101 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0508 10:21:08.628580   16101 fix.go:56] fixHost completed within 5.147085415s
I0508 10:21:08.628617   16101 start.go:83] releasing machines lock for "minikube", held for 5.147155255s
I0508 10:21:08.628763   16101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0508 10:21:08.657768   16101 ssh_runner.go:195] Run: cat /version.json
I0508 10:21:08.657841   16101 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0508 10:21:08.657847   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.657892   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:08.679452   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:08.680446   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:08.761531   16101 ssh_runner.go:195] Run: systemctl --version
I0508 10:21:09.248266   16101 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0508 10:21:09.253873   16101 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0508 10:21:09.291834   16101 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0508 10:21:09.291917   16101 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0508 10:21:09.305787   16101 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0508 10:21:09.305825   16101 start.go:472] detecting cgroup driver to use...
I0508 10:21:09.305885   16101 detect.go:199] detected "systemd" cgroup driver on host os
I0508 10:21:09.306087   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0508 10:21:09.338255   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0508 10:21:09.358488   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0508 10:21:09.372529   16101 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0508 10:21:09.372634   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0508 10:21:09.393199   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0508 10:21:09.406972   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0508 10:21:09.425021   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0508 10:21:09.440026   16101 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0508 10:21:09.456524   16101 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0508 10:21:09.473158   16101 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0508 10:21:09.483777   16101 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0508 10:21:09.493464   16101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 10:21:09.620394   16101 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0508 10:21:09.732623   16101 start.go:472] detecting cgroup driver to use...
I0508 10:21:09.732670   16101 detect.go:199] detected "systemd" cgroup driver on host os
I0508 10:21:09.732735   16101 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0508 10:21:09.747950   16101 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0508 10:21:09.748021   16101 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0508 10:21:09.761775   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0508 10:21:09.781948   16101 ssh_runner.go:195] Run: which cri-dockerd
I0508 10:21:09.785105   16101 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0508 10:21:09.795595   16101 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0508 10:21:09.818266   16101 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0508 10:21:09.903712   16101 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0508 10:21:09.982046   16101 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0508 10:21:09.982121   16101 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0508 10:21:10.000270   16101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 10:21:10.061784   16101 ssh_runner.go:195] Run: sudo systemctl restart docker
I0508 10:21:12.200388   16101 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.138571609s)
I0508 10:21:12.200463   16101 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0508 10:21:12.294276   16101 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0508 10:21:12.386710   16101 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0508 10:21:12.463504   16101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 10:21:12.552710   16101 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0508 10:21:12.594136   16101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0508 10:21:12.673581   16101 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0508 10:21:12.891299   16101 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0508 10:21:12.891401   16101 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0508 10:21:12.897516   16101 start.go:540] Will wait 60s for crictl version
I0508 10:21:12.897563   16101 ssh_runner.go:195] Run: which crictl
I0508 10:21:12.901070   16101 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0508 10:21:13.013459   16101 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0508 10:21:13.013516   16101 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0508 10:21:13.083606   16101 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0508 10:21:13.105225   16101 out.go:204] üê≥  –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è Kubernetes v1.28.3 –Ω–∞ Docker 24.0.7 ...
I0508 10:21:13.105802   16101 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0508 10:21:13.125933   16101 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0508 10:21:13.130520   16101 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0508 10:21:13.144817   16101 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0508 10:21:13.144853   16101 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0508 10:21:13.165026   16101 docker.go:671] Got preloaded images: -- stdout --
quay.io/prometheus/prometheus:v2.51.2
<none>:<none>
httpd:latest
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
grafana/grafana:10.4.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.72.0
quay.io/prometheus/alertmanager:v0.27.0
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
<none>:<none>
registry.k8s.io/metrics-server/metrics-server:<none>
busybox:latest
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
registry.k8s.io/hpa-example:latest

-- /stdout --
I0508 10:21:13.165036   16101 docker.go:601] Images already preloaded, skipping extraction
I0508 10:21:13.165076   16101 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0508 10:21:13.184587   16101 docker.go:671] Got preloaded images: -- stdout --
quay.io/prometheus/prometheus:v2.51.2
httpd:latest
<none>:<none>
quay.io/prometheus/pushgateway:v1.8.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.12.0
grafana/grafana:10.4.1
quay.io/prometheus-operator/prometheus-config-reloader:v0.72.0
quay.io/prometheus/alertmanager:v0.27.0
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
<none>:<none>
registry.k8s.io/metrics-server/metrics-server:<none>
busybox:latest
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
registry.k8s.io/hpa-example:latest

-- /stdout --
I0508 10:21:13.184598   16101 cache_images.go:84] Images are preloaded, skipping loading
I0508 10:21:13.184638   16101 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0508 10:21:13.337516   16101 cni.go:84] Creating CNI manager for ""
I0508 10:21:13.337535   16101 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 10:21:13.338045   16101 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0508 10:21:13.338086   16101 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0508 10:21:13.338259   16101 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0508 10:21:13.338348   16101 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0508 10:21:13.338450   16101 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0508 10:21:13.349818   16101 binaries.go:44] Found k8s binaries, skipping transfer
I0508 10:21:13.349881   16101 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0508 10:21:13.359447   16101 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0508 10:21:13.383141   16101 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0508 10:21:13.415558   16101 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0508 10:21:13.444397   16101 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0508 10:21:13.448499   16101 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0508 10:21:13.471956   16101 certs.go:56] Setting up /home/ilia/.minikube/profiles/minikube for IP: 192.168.49.2
I0508 10:21:13.471976   16101 certs.go:190] acquiring lock for shared ca certs: {Name:mk414557364d0c015d63ae9ac1fdd90c8071b7c3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 10:21:13.472143   16101 certs.go:199] skipping minikubeCA CA generation: /home/ilia/.minikube/ca.key
I0508 10:21:13.472302   16101 certs.go:199] skipping proxyClientCA CA generation: /home/ilia/.minikube/proxy-client-ca.key
I0508 10:21:13.472362   16101 certs.go:315] skipping minikube-user signed cert generation: /home/ilia/.minikube/profiles/minikube/client.key
I0508 10:21:13.472510   16101 certs.go:315] skipping minikube signed cert generation: /home/ilia/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0508 10:21:13.472649   16101 certs.go:315] skipping aggregator signed cert generation: /home/ilia/.minikube/profiles/minikube/proxy-client.key
I0508 10:21:13.472764   16101 certs.go:437] found cert: /home/ilia/.minikube/certs/home/ilia/.minikube/certs/ca-key.pem (1679 bytes)
I0508 10:21:13.472786   16101 certs.go:437] found cert: /home/ilia/.minikube/certs/home/ilia/.minikube/certs/ca.pem (1074 bytes)
I0508 10:21:13.472806   16101 certs.go:437] found cert: /home/ilia/.minikube/certs/home/ilia/.minikube/certs/cert.pem (1115 bytes)
I0508 10:21:13.472828   16101 certs.go:437] found cert: /home/ilia/.minikube/certs/home/ilia/.minikube/certs/key.pem (1675 bytes)
I0508 10:21:13.473216   16101 ssh_runner.go:362] scp /home/ilia/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0508 10:21:13.519335   16101 ssh_runner.go:362] scp /home/ilia/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0508 10:21:13.549168   16101 ssh_runner.go:362] scp /home/ilia/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0508 10:21:13.581571   16101 ssh_runner.go:362] scp /home/ilia/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0508 10:21:13.620838   16101 ssh_runner.go:362] scp /home/ilia/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0508 10:21:13.647884   16101 ssh_runner.go:362] scp /home/ilia/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0508 10:21:13.687029   16101 ssh_runner.go:362] scp /home/ilia/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0508 10:21:13.726556   16101 ssh_runner.go:362] scp /home/ilia/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0508 10:21:13.772498   16101 ssh_runner.go:362] scp /home/ilia/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0508 10:21:13.811503   16101 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0508 10:21:13.843845   16101 ssh_runner.go:195] Run: openssl version
I0508 10:21:13.857255   16101 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0508 10:21:13.877819   16101 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0508 10:21:13.882701   16101 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Apr 19 09:54 /usr/share/ca-certificates/minikubeCA.pem
I0508 10:21:13.882770   16101 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0508 10:21:13.896503   16101 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0508 10:21:13.909484   16101 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0508 10:21:13.914548   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0508 10:21:13.922161   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0508 10:21:13.935488   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0508 10:21:13.947364   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0508 10:21:13.961257   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0508 10:21:13.975489   16101 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0508 10:21:13.982475   16101 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:3800 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true ingress:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ilia:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0508 10:21:13.982619   16101 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0508 10:21:14.011403   16101 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0508 10:21:14.022097   16101 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0508 10:21:14.022470   16101 kubeadm.go:636] restartCluster start
I0508 10:21:14.022546   16101 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0508 10:21:14.041805   16101 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0508 10:21:14.042899   16101 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0508 10:21:14.050102   16101 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0508 10:21:14.061104   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:14.061160   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:14.077683   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:14.077698   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:14.077760   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:14.099264   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:14.599967   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:14.600039   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:14.624400   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:15.099819   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:15.099988   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:15.124952   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:15.600140   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:15.600233   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:15.621726   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:16.099886   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:16.099971   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:16.124469   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:16.600167   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:16.600302   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:16.616728   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:17.099980   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:17.100069   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:17.123916   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:17.599336   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:17.599427   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:17.615282   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:18.099536   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:18.099642   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:18.124048   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:18.599733   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:18.599803   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:18.623869   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:19.100230   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:19.100319   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:19.121776   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:19.600055   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:19.600140   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:19.624221   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:20.099602   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:20.099678   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:20.121204   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:20.599597   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:20.599729   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:20.623689   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:21.099803   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:21.099913   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:21.121042   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:21.599371   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:21.599537   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:21.624584   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:22.100161   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:22.100234   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:22.123767   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:22.599909   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:22.600003   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:22.624172   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:23.100386   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:23.100467   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:23.115498   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:23.599534   16101 api_server.go:166] Checking apiserver status ...
I0508 10:21:23.599632   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0508 10:21:23.616165   16101 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0508 10:21:24.061520   16101 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0508 10:21:24.061613   16101 kubeadm.go:1128] stopping kube-system containers ...
I0508 10:21:24.061730   16101 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0508 10:21:24.104678   16101 docker.go:469] Stopping containers: [9e7aeef4dbd2 38792610295d 35da81d11624 f26e809f3065 f819b1370b8e b3b64f832a75 515f42e40408 dee3400c5498 a5505cd5efc2 7366f5646d4d b1f62ac02f22 8e9f88fb8334 81659abc60ce 80c316c4bb21 e647c67e1527 8bf93a7d3fa4 467ab08de182 82eec4bdc5f8 3a4baf83c3ed b9c58b410be9 6619df11d0a7 f08877e76b20 46da3942cb74 5e747aa74ea0 a2baaac258f2 0e22170eb34a 9904dad92b27 73e5230f8d53 abc2bc35942a 424fee8ac583]
I0508 10:21:24.104798   16101 ssh_runner.go:195] Run: docker stop 9e7aeef4dbd2 38792610295d 35da81d11624 f26e809f3065 f819b1370b8e b3b64f832a75 515f42e40408 dee3400c5498 a5505cd5efc2 7366f5646d4d b1f62ac02f22 8e9f88fb8334 81659abc60ce 80c316c4bb21 e647c67e1527 8bf93a7d3fa4 467ab08de182 82eec4bdc5f8 3a4baf83c3ed b9c58b410be9 6619df11d0a7 f08877e76b20 46da3942cb74 5e747aa74ea0 a2baaac258f2 0e22170eb34a 9904dad92b27 73e5230f8d53 abc2bc35942a 424fee8ac583
I0508 10:21:24.132100   16101 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0508 10:21:24.144473   16101 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0508 10:21:24.154876   16101 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 May  6 06:31 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 May  7 08:34 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 May  6 06:31 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 May  7 08:34 /etc/kubernetes/scheduler.conf

I0508 10:21:24.154957   16101 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0508 10:21:24.168839   16101 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0508 10:21:24.179962   16101 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0508 10:21:24.190412   16101 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0508 10:21:24.190466   16101 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0508 10:21:24.199578   16101 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0508 10:21:24.210772   16101 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0508 10:21:24.210845   16101 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0508 10:21:24.221176   16101 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0508 10:21:24.231528   16101 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0508 10:21:24.231550   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:24.386386   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:24.983077   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:25.177436   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:25.224739   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:25.268818   16101 api_server.go:52] waiting for apiserver process to appear ...
I0508 10:21:25.268871   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:25.281221   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:25.792480   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:26.292631   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:26.792012   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:26.805711   16101 api_server.go:72] duration metric: took 1.536894769s to wait for apiserver process to appear ...
I0508 10:21:26.805728   16101 api_server.go:88] waiting for apiserver healthz status ...
I0508 10:21:26.805752   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:26.806066   16101 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0508 10:21:26.806097   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:26.806332   16101 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0508 10:21:27.307057   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:29.170611   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0508 10:21:29.170628   16101 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0508 10:21:29.170642   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:29.218710   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0508 10:21:29.218726   16101 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0508 10:21:29.307129   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:29.312266   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0508 10:21:29.312283   16101 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0508 10:21:29.806827   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:29.812529   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0508 10:21:29.812554   16101 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0508 10:21:30.306775   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:30.313833   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0508 10:21:30.313859   16101 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0508 10:21:30.806981   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:30.812716   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0508 10:21:30.822013   16101 api_server.go:141] control plane version: v1.28.3
I0508 10:21:30.822029   16101 api_server.go:131] duration metric: took 4.01629232s to wait for apiserver health ...
I0508 10:21:30.822036   16101 cni.go:84] Creating CNI manager for ""
I0508 10:21:30.822047   16101 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0508 10:21:30.825585   16101 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0508 10:21:30.828669   16101 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0508 10:21:30.839089   16101 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0508 10:21:30.857918   16101 system_pods.go:43] waiting for kube-system pods to appear ...
I0508 10:21:30.864567   16101 system_pods.go:59] 8 kube-system pods found
I0508 10:21:30.864582   16101 system_pods.go:61] "coredns-5dd5756b68-tgx9k" [8f735c18-0f0f-4456-8e9c-847fa6f04f23] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0508 10:21:30.864589   16101 system_pods.go:61] "etcd-minikube" [be31437f-a196-450e-8087-95336e59df4c] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0508 10:21:30.864595   16101 system_pods.go:61] "kube-apiserver-minikube" [3cd027d3-ef06-4feb-9edc-5df4aab9dd42] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0508 10:21:30.864602   16101 system_pods.go:61] "kube-controller-manager-minikube" [c30f1493-5406-44de-bd82-723261b8648a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0508 10:21:30.864609   16101 system_pods.go:61] "kube-proxy-5lj79" [94aef98f-cad5-4d2f-9209-c9d1f8f235ec] Running
I0508 10:21:30.864616   16101 system_pods.go:61] "kube-scheduler-minikube" [dc2fecac-16c4-4696-9905-58fbc9a70cac] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0508 10:21:30.864623   16101 system_pods.go:61] "metrics-server-7c66d45ddc-zpqcm" [1725f6f7-cc93-45f1-8191-0500f5ed3f6a] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0508 10:21:30.864628   16101 system_pods.go:61] "storage-provisioner" [3fb6ff71-771c-41e0-ac0b-b42e903ea504] Running
I0508 10:21:30.864637   16101 system_pods.go:74] duration metric: took 6.706533ms to wait for pod list to return data ...
I0508 10:21:30.864645   16101 node_conditions.go:102] verifying NodePressure condition ...
I0508 10:21:30.868825   16101 node_conditions.go:122] node storage ephemeral capacity is 51288992Ki
I0508 10:21:30.868840   16101 node_conditions.go:123] node cpu capacity is 12
I0508 10:21:30.868860   16101 node_conditions.go:105] duration metric: took 4.206023ms to run NodePressure ...
I0508 10:21:30.868880   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0508 10:21:31.036107   16101 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0508 10:21:31.046661   16101 ops.go:34] apiserver oom_adj: -16
I0508 10:21:31.046673   16101 kubeadm.go:640] restartCluster took 17.024190471s
I0508 10:21:31.046681   16101 kubeadm.go:406] StartCluster complete in 17.064221178s
I0508 10:21:31.046700   16101 settings.go:142] acquiring lock: {Name:mk7298d938a38b4de8670d5d03bfe726c58a0401 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 10:21:31.046779   16101 settings.go:150] Updating kubeconfig:  /home/ilia/.kube/config
I0508 10:21:31.047181   16101 lock.go:35] WriteFile acquiring /home/ilia/.kube/config: {Name:mkd9b6b147b0e3923e6d690ac3cd3da2f575350c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0508 10:21:31.047338   16101 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0508 10:21:31.047365   16101 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0508 10:21:31.047417   16101 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0508 10:21:31.047425   16101 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0508 10:21:31.047430   16101 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0508 10:21:31.047436   16101 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0508 10:21:31.047438   16101 addons.go:240] addon storage-provisioner should already be in state true
I0508 10:21:31.047436   16101 addons.go:69] Setting ingress=true in profile "minikube"
I0508 10:21:31.047449   16101 addons.go:231] Setting addon ingress=true in "minikube"
W0508 10:21:31.047456   16101 addons.go:240] addon ingress should already be in state true
I0508 10:21:31.047473   16101 host.go:66] Checking if "minikube" exists ...
I0508 10:21:31.047503   16101 host.go:66] Checking if "minikube" exists ...
I0508 10:21:31.047607   16101 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0508 10:21:31.047666   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.047760   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.047842   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.048054   16101 addons.go:69] Setting metrics-server=true in profile "minikube"
I0508 10:21:31.048072   16101 addons.go:69] Setting dashboard=true in profile "minikube"
I0508 10:21:31.048092   16101 addons.go:231] Setting addon metrics-server=true in "minikube"
I0508 10:21:31.048098   16101 addons.go:231] Setting addon dashboard=true in "minikube"
W0508 10:21:31.048103   16101 addons.go:240] addon metrics-server should already be in state true
W0508 10:21:31.048104   16101 addons.go:240] addon dashboard should already be in state true
I0508 10:21:31.048155   16101 host.go:66] Checking if "minikube" exists ...
I0508 10:21:31.048155   16101 host.go:66] Checking if "minikube" exists ...
I0508 10:21:31.048546   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.048546   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.051593   16101 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0508 10:21:31.051636   16101 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0508 10:21:31.053785   16101 out.go:177] üîé  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Kubernetes –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è ...
I0508 10:21:31.055567   16101 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0508 10:21:31.078054   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ docker.io/kubernetesui/dashboard:v2.7.0
I0508 10:21:31.077183   16101 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0508 10:21:31.080911   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ gcr.io/k8s-minikube/storage-provisioner:v5
I0508 10:21:31.081251   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/metrics-server/metrics-server:v0.6.4
W0508 10:21:31.081269   16101 addons.go:240] addon default-storageclass should already be in state true
I0508 10:21:31.082734   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0508 10:21:31.083077   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ docker.io/kubernetesui/metrics-scraper:v1.0.8
I0508 10:21:31.083119   16101 host.go:66] Checking if "minikube" exists ...
I0508 10:21:31.086114   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20231011-8b53cabe0
I0508 10:21:31.087936   16101 addons.go:423] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0508 10:21:31.088647   16101 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0508 10:21:31.088885   16101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0508 10:21:31.090518   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0508 10:21:31.090528   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0508 10:21:31.092204   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0508 10:21:31.092213   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0508 10:21:31.097527   16101 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ registry.k8s.io/ingress-nginx/controller:v1.9.4
I0508 10:21:31.092272   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:31.092272   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:31.092298   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:31.101007   16101 addons.go:423] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0508 10:21:31.101019   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16103 bytes)
I0508 10:21:31.101080   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:31.118488   16101 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0508 10:21:31.118503   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0508 10:21:31.118571   16101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0508 10:21:31.130082   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:31.130429   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:31.131127   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:31.131915   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:31.142656   16101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/ilia/.minikube/machines/minikube/id_rsa Username:docker}
I0508 10:21:31.238058   16101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0508 10:21:31.238058   16101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0508 10:21:31.238233   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0508 10:21:31.238242   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0508 10:21:31.239104   16101 addons.go:423] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0508 10:21:31.239117   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0508 10:21:31.268229   16101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0508 10:21:31.268318   16101 addons.go:423] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0508 10:21:31.268332   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0508 10:21:31.268382   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0508 10:21:31.268390   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0508 10:21:31.298082   16101 addons.go:423] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0508 10:21:31.298098   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0508 10:21:31.302398   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0508 10:21:31.302416   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0508 10:21:31.326880   16101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0508 10:21:31.328801   16101 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0508 10:21:31.328810   16101 api_server.go:52] waiting for apiserver process to appear ...
I0508 10:21:31.328866   16101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0508 10:21:31.330002   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0508 10:21:31.330014   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0508 10:21:31.354672   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0508 10:21:31.354685   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0508 10:21:31.386941   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0508 10:21:31.386960   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0508 10:21:31.417594   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0508 10:21:31.417609   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0508 10:21:31.451091   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0508 10:21:31.451107   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0508 10:21:31.480698   16101 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0508 10:21:31.480717   16101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0508 10:21:31.516136   16101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0508 10:21:50.150699   16101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (18.912617589s)
I0508 10:21:50.150714   16101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (18.912635538s)
I0508 10:21:50.150718   16101 addons.go:467] Verifying addon ingress=true in "minikube"
I0508 10:21:50.150743   16101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (18.882497697s)
I0508 10:21:50.152189   16101 out.go:177] üîé  Verifying ingress addon...
I0508 10:21:50.150827   16101 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (18.821948167s)
I0508 10:21:50.150918   16101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (18.634750026s)
I0508 10:21:50.151095   16101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (18.823896445s)
I0508 10:21:50.154521   16101 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0508 10:21:50.154521   16101 api_server.go:72] duration metric: took 19.102849895s to wait for apiserver process to appear ...
I0508 10:21:50.154541   16101 api_server.go:88] waiting for apiserver healthz status ...
I0508 10:21:50.154565   16101 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0508 10:21:50.155643   16101 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0508 10:21:50.155731   16101 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0508 10:21:50.159514   16101 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0508 10:21:50.160406   16101 api_server.go:141] control plane version: v1.28.3
I0508 10:21:50.160420   16101 api_server.go:131] duration metric: took 5.871378ms to wait for apiserver health ...
I0508 10:21:50.160428   16101 system_pods.go:43] waiting for kube-system pods to appear ...
I0508 10:21:50.161041   16101 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0508 10:21:50.161053   16101 kapi.go:107] duration metric: took 5.323897ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0508 10:21:50.161966   16101 out.go:177] üåü  –í–∫–ª—é—á–µ–Ω–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è: storage-provisioner, metrics-server, dashboard, default-storageclass, ingress
I0508 10:21:50.163560   16101 addons.go:502] enable addons completed in 19.116193752s: enabled=[storage-provisioner metrics-server dashboard default-storageclass ingress]
I0508 10:21:50.165266   16101 system_pods.go:59] 8 kube-system pods found
I0508 10:21:50.165282   16101 system_pods.go:61] "coredns-5dd5756b68-tgx9k" [8f735c18-0f0f-4456-8e9c-847fa6f04f23] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0508 10:21:50.165288   16101 system_pods.go:61] "etcd-minikube" [be31437f-a196-450e-8087-95336e59df4c] Running
I0508 10:21:50.165293   16101 system_pods.go:61] "kube-apiserver-minikube" [3cd027d3-ef06-4feb-9edc-5df4aab9dd42] Running
I0508 10:21:50.165298   16101 system_pods.go:61] "kube-controller-manager-minikube" [c30f1493-5406-44de-bd82-723261b8648a] Running
I0508 10:21:50.165302   16101 system_pods.go:61] "kube-proxy-5lj79" [94aef98f-cad5-4d2f-9209-c9d1f8f235ec] Running
I0508 10:21:50.165307   16101 system_pods.go:61] "kube-scheduler-minikube" [dc2fecac-16c4-4696-9905-58fbc9a70cac] Running
I0508 10:21:50.165313   16101 system_pods.go:61] "metrics-server-7c66d45ddc-zpqcm" [1725f6f7-cc93-45f1-8191-0500f5ed3f6a] Running / Ready:ContainersNotReady (containers with unready status: [metrics-server]) / ContainersReady:ContainersNotReady (containers with unready status: [metrics-server])
I0508 10:21:50.165318   16101 system_pods.go:61] "storage-provisioner" [3fb6ff71-771c-41e0-ac0b-b42e903ea504] Running
I0508 10:21:50.165325   16101 system_pods.go:74] duration metric: took 4.888858ms to wait for pod list to return data ...
I0508 10:21:50.165334   16101 kubeadm.go:581] duration metric: took 19.113669566s to wait for : map[apiserver:true system_pods:true] ...
I0508 10:21:50.165373   16101 node_conditions.go:102] verifying NodePressure condition ...
I0508 10:21:50.168007   16101 node_conditions.go:122] node storage ephemeral capacity is 51288992Ki
I0508 10:21:50.168021   16101 node_conditions.go:123] node cpu capacity is 12
I0508 10:21:50.168033   16101 node_conditions.go:105] duration metric: took 2.652555ms to run NodePressure ...
I0508 10:21:50.168042   16101 start.go:228] waiting for startup goroutines ...
I0508 10:21:50.168048   16101 start.go:233] waiting for cluster config update ...
I0508 10:21:50.168057   16101 start.go:242] writing updated cluster config ...
I0508 10:21:50.168280   16101 ssh_runner.go:195] Run: rm -f paused
I0508 10:21:50.209807   16101 start.go:600] kubectl: 1.20.2, cluster: 1.28.3 (minor skew: 8)
I0508 10:21:50.223357   16101 out.go:177] 
W0508 10:21:50.225456   16101 out.go:239] ‚ùó  /usr/bin/kubectl is version 1.20.2, which may have incompatibilities with Kubernetes 1.28.3.
I0508 10:21:50.227229   16101 out.go:177]     ‚ñ™ Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I0508 10:21:50.229195   16101 out.go:177] üèÑ  –ì–æ—Ç–æ–≤–æ! kubectl –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ "minikube" –∏ "default" –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º—ë–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

* 
* ==> Docker <==
* May 08 07:21:32 minikube cri-dockerd[1109]: time="2024-05-08T07:21:32Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
May 08 07:21:33 minikube cri-dockerd[1109]: time="2024-05-08T07:21:33Z" level=info msg="Stop pulling image registry.k8s.io/hpa-example:latest: Status: Image is up to date for registry.k8s.io/hpa-example:latest"
May 08 07:21:57 minikube dockerd[872]: time="2024-05-08T07:21:57.547636431Z" level=info msg="ignoring event" container=8fd17d19de2e1fca45e6960e4162bdb43f1cbcd0844d036862175b31be05f9fe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:00 minikube dockerd[872]: time="2024-05-08T07:22:00.576869155Z" level=info msg="ignoring event" container=ca6db11709b66251a68719a5256eeb32c23c4505c101ba0f9a27605235733a31 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:00 minikube dockerd[872]: time="2024-05-08T07:22:00.703255793Z" level=info msg="ignoring event" container=80365797c769db09e64f8963b03c71bb4c716503d2e2d0701a0ff5af2f199f68 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:00 minikube dockerd[872]: time="2024-05-08T07:22:00.985637325Z" level=info msg="ignoring event" container=6a69711a9afeba9e87f8bebde70eedef4ae588c019e7cc155a3aeb3ea7fa64d5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:01 minikube cri-dockerd[1109]: time="2024-05-08T07:22:01Z" level=error msg="Error response from daemon: No such container: 38792610295df427478b430b4cd94ff54d9337ebe58cf4c38b722427fb6af8d8 Failed to get stats from container 38792610295df427478b430b4cd94ff54d9337ebe58cf4c38b722427fb6af8d8"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.367741763Z" level=info msg="ignoring event" container=9845b59e646f42a4fc3d343529f17586cdb59159a6ca413abb2232a32b561c9c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.396963526Z" level=info msg="ignoring event" container=3afebb469f356301cc99a6348fc317e669379e52628eeb0e7b8fcf2e0c2f8ce5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.398100117Z" level=info msg="ignoring event" container=34b2853002aeb4750c78331ccb3425bf569eeeea2b3f2108a071e269b6235574 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.400136886Z" level=info msg="ignoring event" container=080eba9bc6c6353e1d0b836695a5fb299f5c324c94418f4920daf2377def945a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.429949992Z" level=info msg="ignoring event" container=8224a47b878697f6776109d51e939661556559348dc7a590f51c563c1f517328 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.477426899Z" level=info msg="ignoring event" container=cbf0acd2ce60353463ea5228c75dfe3f8c429ab845aa7cc083543c05ec4ed5e3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.477702840Z" level=info msg="ignoring event" container=82113686b767fbf0d2aa265a1f4e21d0066caa7581ba638ff75577b24def8045 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.555794487Z" level=info msg="ignoring event" container=e5b29bd43a8ac932178edbb15cbf30d8a5b8706a387d5fd89112218703725aa4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.555835833Z" level=info msg="ignoring event" container=9e1cb43e3cb3bcc8631a6aa7306599b0b4956ed694c3fdf4abc0c68fab927c97 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.707015029Z" level=info msg="ignoring event" container=bb0c02ba66a0322f789bc2776b2da89f03e06844747b9e653561ff20a7792ba0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.825789045Z" level=info msg="ignoring event" container=363c9e5b4aca6dc4af9c780c7d0fc89e0e65fb6ac4e3cebe39031810130f36b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:14 minikube cri-dockerd[1109]: time="2024-05-08T07:22:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/07bbca05a78ef8766d86c0acc091d76aa08df232c3f4e595e6fdd5d07d829077/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:14 minikube cri-dockerd[1109]: time="2024-05-08T07:22:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f45dad69dba64b5a79c567e30976ee1a616715f6f656c73b4c7f0d41e5cf605d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:14 minikube cri-dockerd[1109]: time="2024-05-08T07:22:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/57571a4fd6225b874941d61428a1534f46534165d90d113ec1243c7754ad3f24/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:14 minikube cri-dockerd[1109]: time="2024-05-08T07:22:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b7874051c8542b7e3c25b0ac91ea9bbc8338be9552d5df205784883241e7c8d4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:14 minikube cri-dockerd[1109]: time="2024-05-08T07:22:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e51bc9175148a425a9ad7e9cb4f7411965d39364a1131b5b9c668b9f58b9900b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:14 minikube dockerd[872]: time="2024-05-08T07:22:14.927392267Z" level=info msg="ignoring event" container=ff8a7fcf35d4da3d94cc8be66f06d63bdf3907f72d9897858319e5d6a86571cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:15 minikube dockerd[872]: time="2024-05-08T07:22:15.008003772Z" level=warning msg="Published ports are discarded when using host network mode"
May 08 07:22:15 minikube dockerd[872]: time="2024-05-08T07:22:15.012921057Z" level=info msg="ignoring event" container=e57b95268beb58e3fb2377e4330d35e3f485bf92d1b0bc1b3a0001109ae1a939 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:15 minikube dockerd[872]: time="2024-05-08T07:22:15.034185503Z" level=warning msg="Published ports are discarded when using host network mode"
May 08 07:22:15 minikube cri-dockerd[1109]: time="2024-05-08T07:22:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e827b7fa54632c73561026cde3923d045dc1ca19d9fae367cce3575720399260/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 08 07:22:15 minikube cri-dockerd[1109]: time="2024-05-08T07:22:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"prometheus-prometheus-pushgateway-568fbf799-kncqr_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
May 08 07:22:15 minikube dockerd[872]: time="2024-05-08T07:22:15.174985792Z" level=info msg="ignoring event" container=ea4abecad9dc48ebee54ad3950dc6244bae9c06822f37bdef7bd3eb95647151f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:15 minikube dockerd[872]: time="2024-05-08T07:22:15.175031468Z" level=info msg="ignoring event" container=a20e08aae3fcf95ab70b07605318f69afc79c801836e5d046990b53dd6deea2e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:15 minikube cri-dockerd[1109]: time="2024-05-08T07:22:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5b176ee1bdc9fa77b92d03e002132a5d5c98d0ba55fb7e8e3afd52caa4b0185/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:22:16 minikube cri-dockerd[1109]: time="2024-05-08T07:22:16Z" level=info msg="Stop pulling image registry.k8s.io/hpa-example:latest: Status: Image is up to date for registry.k8s.io/hpa-example:latest"
May 08 07:22:44 minikube dockerd[872]: time="2024-05-08T07:22:44.322186911Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=7285f22391059e9518966e8dbc69fee4e09236b02f860b476dd010249896a403
May 08 07:22:44 minikube dockerd[872]: time="2024-05-08T07:22:44.351426988Z" level=info msg="ignoring event" container=7285f22391059e9518966e8dbc69fee4e09236b02f860b476dd010249896a403 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:44 minikube dockerd[872]: time="2024-05-08T07:22:44.362140790Z" level=warning msg="failed to close stdin: task 7285f22391059e9518966e8dbc69fee4e09236b02f860b476dd010249896a403 not found: not found"
May 08 07:22:44 minikube dockerd[872]: time="2024-05-08T07:22:44.637072253Z" level=info msg="ignoring event" container=6edf275bbf4fd5fd0456277d65f0f1640eb72c3c9e1e22eea92dc7d848035cda module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.136066014Z" level=info msg="ignoring event" container=7f8108d9be6e23df6d71d3f3b1f01f6e6da4a9240731da6d52e11e73ad7abb45 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.138350231Z" level=info msg="ignoring event" container=912683ffd491217cc5776ea035bcfd6797c9406a5b3a3ae94ddc20087d06ae25 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.141862600Z" level=info msg="ignoring event" container=d143d1c2d14f2a18a633155b509d7210e024dab1f296f36a97fe94c512b894f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.147910333Z" level=info msg="ignoring event" container=ccfecda62e0524de6f6ad367ba44a9c5b04d276e763545ce2926a22ea4ede8b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.147959431Z" level=info msg="ignoring event" container=412e6d5c56277fa7b3598bee420a0bbaa1e9be3d5f88160735fcd1f297e5fb8d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.161197075Z" level=info msg="ignoring event" container=9c95a1607dafdf655699b2592d59ff681c07fe2bc73cdcea4c19cc9debd752bb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.445682741Z" level=info msg="ignoring event" container=57571a4fd6225b874941d61428a1534f46534165d90d113ec1243c7754ad3f24 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube cri-dockerd[1109]: time="2024-05-08T07:22:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"prometheus-prometheus-pushgateway-568fbf799-29hgd_default\": unexpected command output nsenter: cannot open /proc/8066/ns/net: No such file or directory\n with error: exit status 1"
May 08 07:22:53 minikube cri-dockerd[1109]: time="2024-05-08T07:22:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"grafana-55dc9f456-kstnz_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.608588598Z" level=info msg="ignoring event" container=f45dad69dba64b5a79c567e30976ee1a616715f6f656c73b4c7f0d41e5cf605d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.682401212Z" level=info msg="ignoring event" container=b7874051c8542b7e3c25b0ac91ea9bbc8338be9552d5df205784883241e7c8d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.797233970Z" level=info msg="ignoring event" container=07bbca05a78ef8766d86c0acc091d76aa08df232c3f4e595e6fdd5d07d829077 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:22:53 minikube cri-dockerd[1109]: time="2024-05-08T07:22:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"prometheus-server-579dc9cfdf-gt8gg_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
May 08 07:22:53 minikube dockerd[872]: time="2024-05-08T07:22:53.872399262Z" level=info msg="ignoring event" container=e51bc9175148a425a9ad7e9cb4f7411965d39364a1131b5b9c668b9f58b9900b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:24:36 minikube dockerd[872]: time="2024-05-08T07:24:36.818469849Z" level=info msg="ignoring event" container=9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:24:37 minikube dockerd[872]: time="2024-05-08T07:24:37.087502228Z" level=info msg="ignoring event" container=e5b176ee1bdc9fa77b92d03e002132a5d5c98d0ba55fb7e8e3afd52caa4b0185 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:24:47 minikube dockerd[872]: time="2024-05-08T07:24:47.782268555Z" level=info msg="ignoring event" container=9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:24:47 minikube dockerd[872]: time="2024-05-08T07:24:47.868472529Z" level=info msg="ignoring event" container=e827b7fa54632c73561026cde3923d045dc1ca19d9fae367cce3575720399260 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 08 07:30:31 minikube cri-dockerd[1109]: time="2024-05-08T07:30:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fb64a1d2383bbc5c3e0efa116151886f3b8314c38390c3601579d882d58d57f4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:30:31 minikube cri-dockerd[1109]: time="2024-05-08T07:30:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a4c7b44385dd96358d26c681902e8a084ff7d24aaf8fcca2c2bfe146a7297b4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 08 07:30:39 minikube cri-dockerd[1109]: time="2024-05-08T07:30:39Z" level=info msg="Stop pulling image ilia1234559/task31:nginx-1: Status: Downloaded newer image for ilia1234559/task31:nginx-1"
May 08 07:30:51 minikube cri-dockerd[1109]: time="2024-05-08T07:30:51Z" level=info msg="Pulling image ilia1234559/task31:apache: 71e5060f3937: Extracting [============================>                      ]  52.92MB/91.64MB"
May 08 07:30:56 minikube cri-dockerd[1109]: time="2024-05-08T07:30:56Z" level=info msg="Stop pulling image ilia1234559/task31:apache: Status: Downloaded newer image for ilia1234559/task31:apache"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                        CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
5494d7e9a84fb       ilia1234559/task31@sha256:59074170345d39d14297182162e1e6f52d3d1cbfa54c917930776b02cb5f40c4   About a minute ago   Running             task31-apache-web           0                   4a4c7b44385dd       task31-apache-deployment-55478b4d75-9b7mp
84434ac532e0d       ilia1234559/task31@sha256:66821978049905cd57a12a610082d8c7bda1f9e0b72ed235c27f16031edef8b3   About a minute ago   Running             task31-nginx-web            0                   fb64a1d2383bb       task31-nginx-deployment-744d867495-nt4b8
744a9d3bbfbbc       07655ddf2eebe                                                                                10 minutes ago       Running             kubernetes-dashboard        6                   6323ea5aa35e9       kubernetes-dashboard-8694d4445c-fmz9j
23b935cbf47d4       6e38f40d628db                                                                                10 minutes ago       Running             storage-provisioner         16                  c46423786f6ba       storage-provisioner
72c9dff82cf95       a608c686bac93                                                                                10 minutes ago       Running             metrics-server              5                   7747b957d50d1       metrics-server-7c66d45ddc-zpqcm
b76de3768467d       ead0a4a53df89                                                                                11 minutes ago       Running             coredns                     10                  08131002c3de0       coredns-5dd5756b68-tgx9k
7b0c05a255a17       5aa0bf4798fa2                                                                                11 minutes ago       Running             controller                  11                  b59c8cac640f7       ingress-nginx-controller-7c6974c4d8-6hb77
80365797c769d       6e38f40d628db                                                                                11 minutes ago       Exited              storage-provisioner         15                  c46423786f6ba       storage-provisioner
21155605e93e3       bfc896cf80fba                                                                                11 minutes ago       Running             kube-proxy                  8                   46c66ae2b46be       kube-proxy-5lj79
e391995d6f57d       115053965e86b                                                                                11 minutes ago       Running             dashboard-metrics-scraper   3                   7abf0eee77b66       dashboard-metrics-scraper-7fd5cb4ddc-759q7
6a69711a9afeb       a608c686bac93                                                                                11 minutes ago       Exited              metrics-server              4                   7747b957d50d1       metrics-server-7c66d45ddc-zpqcm
ca6db11709b66       07655ddf2eebe                                                                                11 minutes ago       Exited              kubernetes-dashboard        5                   6323ea5aa35e9       kubernetes-dashboard-8694d4445c-fmz9j
75f7d6f9cfc49       6d1b4fd1b182d                                                                                11 minutes ago       Running             kube-scheduler              8                   337e2f7db9ff0       kube-scheduler-minikube
b89fbdcb66249       5374347291230                                                                                11 minutes ago       Running             kube-apiserver              8                   8af56eaa73ee0       kube-apiserver-minikube
ca04ccf08bf0d       73deb9a3f7025                                                                                11 minutes ago       Running             etcd                        8                   cd8a636982bdc       etcd-minikube
a75c46f7d1af1       10baa1ca17068                                                                                11 minutes ago       Running             kube-controller-manager     8                   fc98a85c3f63f       kube-controller-manager-minikube
0a4286e4ba092       5aa0bf4798fa2                                                                                23 hours ago         Exited              controller                  10                  8afc586132fae       ingress-nginx-controller-7c6974c4d8-6hb77
35da81d116247       ead0a4a53df89                                                                                23 hours ago         Exited              coredns                     9                   a5505cd5efc2c       coredns-5dd5756b68-tgx9k
f819b1370b8ee       bfc896cf80fba                                                                                23 hours ago         Exited              kube-proxy                  7                   dee3400c54989       kube-proxy-5lj79
19fd7b6964e67       115053965e86b                                                                                23 hours ago         Exited              dashboard-metrics-scraper   2                   b12b385eb4404       dashboard-metrics-scraper-7fd5cb4ddc-759q7
b1f62ac02f22a       73deb9a3f7025                                                                                23 hours ago         Exited              etcd                        7                   e647c67e1527a       etcd-minikube
8e9f88fb8334e       6d1b4fd1b182d                                                                                23 hours ago         Exited              kube-scheduler              7                   8bf93a7d3fa44       kube-scheduler-minikube
81659abc60ced       5374347291230                                                                                23 hours ago         Exited              kube-apiserver              7                   82eec4bdc5f81       kube-apiserver-minikube
80c316c4bb213       10baa1ca17068                                                                                23 hours ago         Exited              kube-controller-manager     7                   467ab08de1827       kube-controller-manager-minikube
fb676ae964035       1ebff0f9671bc                                                                                2 weeks ago          Exited              patch                       0                   69ee5f1612798       ingress-nginx-admission-patch-mz77m
0913cdb71418f       1ebff0f9671bc                                                                                2 weeks ago          Exited              create                      0                   52f64a8a74a61       ingress-nginx-admission-create-hvvbx

* 
* ==> controller_ingress [0a4286e4ba09] <==
* W0507 08:34:48.874505       7 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0507 08:34:48.874779       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0507 08:35:19.994759       7 main.go:246] Initial connection to the Kubernetes API server was retried 1 times.
I0507 08:35:19.994799       7 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0507 08:35:20.136101       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0507 08:35:20.160919       7 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0507 08:35:20.176092       7 nginx.go:260] "Starting NGINX Ingress controller"
I0507 08:35:20.182396       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"a2ffd6e7-efef-4803-b89d-0caeba84035d", APIVersion:"v1", ResourceVersion:"488", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0507 08:35:20.185724       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"6e2eefee-7f26-428a-b348-f16ddb7a1037", APIVersion:"v1", ResourceVersion:"489", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0507 08:35:20.185774       7 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"98199d50-37fb-4790-be41-b785c5db91d9", APIVersion:"v1", ResourceVersion:"490", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0507 08:35:21.378198       7 nginx.go:303] "Starting NGINX process"
I0507 08:35:21.378323       7 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0507 08:35:21.378727       7 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0507 08:35:21.379026       7 controller.go:190] "Configuration changes detected, backend reload required"
I0507 08:35:21.384564       7 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0507 08:35:21.384634       7 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-6hb77"
I0507 08:35:21.387344       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-6hb77" node="minikube"
I0507 08:35:21.473461       7 controller.go:210] "Backend successfully reloaded"
I0507 08:35:21.473594       7 controller.go:221] "Initial sync, sleeping for 1 second"
I0507 08:35:21.473693       7 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-6hb77", UID:"0884e422-e4fa-4aeb-9d58-be4a0f08ad70", APIVersion:"v1", ResourceVersion:"23621", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0507 09:48:51.534333       7 sigterm.go:36] "Received SIGTERM, shutting down"
I0507 09:48:51.534365       7 nginx.go:379] "Shutting down controller queues"
E0507 09:48:52.844141       7 main.go:49] "Error getting node" err="Get \"https://10.96.0.1:443/api/v1/nodes/minikube\": dial tcp 10.96.0.1:443: connect: connection refused - error from a previous attempt: unexpected EOF" name="minikube"
I0507 09:48:52.844674       7 status.go:135] "removing value from ingress status" address=[{}]
I0507 09:48:52.844733       7 nginx.go:387] "Stopping admission controller"
E0507 09:48:52.844790       7 nginx.go:326] "Error listening for TLS connections" err="http: Server closed"
I0507 09:48:52.844826       7 nginx.go:395] "Stopping NGINX process"
2024/05/07 09:48:52 [notice] 433#433: signal process started
2024/05/07 09:48:52 [error] 433#433: open() "/tmp/nginx/nginx.pid" failed (2: No such file or directory)
nginx: [error] open() "/tmp/nginx/nginx.pid" failed (2: No such file or directory)
W0507 09:48:52.895180       7 sigterm.go:40] Error during shutdown: exit status 1
I0507 09:48:52.895216       7 sigterm.go:44] Handled quit, delaying controller exit for 10 seconds
E0507 09:48:57.737382       7 leaderelection.go:327] error retrieving resource lock ingress-nginx/ingress-nginx-leader: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader": dial tcp 10.96.0.1:443: connect: connection refused
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> controller_ingress [7b0c05a255a1] <==
* W0508 07:21:30.906735       6 client_config.go:618] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0508 07:21:30.906986       6 main.go:205] "Creating API client" host="https://10.96.0.1:443"
W0508 07:22:02.064566       6 main.go:246] Initial connection to the Kubernetes API server was retried 1 times.
I0508 07:22:02.064620       6 main.go:249] "Running in Kubernetes cluster" major="1" minor="28" git="v1.28.3" state="clean" commit="a8a1abc25cad87333840cd7d54be2efaf31a3177" platform="linux/amd64"
I0508 07:22:02.190542       6 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0508 07:22:02.225702       6 ssl.go:536] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0508 07:22:02.241451       6 nginx.go:260] "Starting NGINX Ingress controller"
I0508 07:22:02.250272       6 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"a2ffd6e7-efef-4803-b89d-0caeba84035d", APIVersion:"v1", ResourceVersion:"488", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0508 07:22:02.254619       6 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"6e2eefee-7f26-428a-b348-f16ddb7a1037", APIVersion:"v1", ResourceVersion:"489", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0508 07:22:02.254698       6 event.go:298] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"98199d50-37fb-4790-be41-b785c5db91d9", APIVersion:"v1", ResourceVersion:"490", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0508 07:22:03.443515       6 nginx.go:303] "Starting NGINX process"
I0508 07:22:03.443628       6 leaderelection.go:245] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0508 07:22:03.444121       6 nginx.go:323] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0508 07:22:03.446542       6 controller.go:190] "Configuration changes detected, backend reload required"
I0508 07:22:03.452277       6 leaderelection.go:255] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0508 07:22:03.452369       6 status.go:84] "New leader elected" identity="ingress-nginx-controller-7c6974c4d8-6hb77"
I0508 07:22:03.457649       6 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-7c6974c4d8-6hb77" node="minikube"
I0508 07:22:03.538664       6 controller.go:210] "Backend successfully reloaded"
I0508 07:22:03.538782       6 controller.go:221] "Initial sync, sleeping for 1 second"
I0508 07:22:03.538915       6 event.go:298] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-7c6974c4d8-6hb77", UID:"0884e422-e4fa-4aeb-9d58-be4a0f08ad70", APIVersion:"v1", ResourceVersion:"28060", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.9.4
  Build:         846d251814a09d8a5d8d28e2e604bfc7749bcb49
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.21.6

-------------------------------------------------------------------------------


* 
* ==> coredns [35da81d11624] <==
* [INFO] 10.244.0.70:41717 - 60521 "A IN secure.gravatar.com. udp 48 false 1232" NOERROR qr,aa,rd,ra 72 0.000066699s
[INFO] 10.244.0.70:40826 - 24029 "AAAA IN secure.gravatar.com. udp 48 false 1232" NOERROR qr,aa,rd,ra 84 0.000119289s
[INFO] 10.244.0.70:33412 - 56844 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000302342s
[INFO] 10.244.0.70:45257 - 42975 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000399072s
[INFO] 10.244.0.70:51775 - 17288 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000170552s
[INFO] 10.244.0.70:47411 - 2549 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000255688s
[INFO] 10.244.0.70:39304 - 42645 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000167269s
[INFO] 10.244.0.70:44332 - 6723 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000163708s
[INFO] 10.244.0.70:52682 - 13311 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.008553793s
[INFO] 10.244.0.70:43513 - 59087 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.011960365s
[INFO] 10.244.0.70:41035 - 22209 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000285999s
[INFO] 10.244.0.70:51274 - 19053 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.00038231s
[INFO] 10.244.0.70:41914 - 62082 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000245142s
[INFO] 10.244.0.70:34616 - 11390 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000380703s
[INFO] 10.244.0.70:36367 - 7451 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000205612s
[INFO] 10.244.0.70:35888 - 37504 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000283764s
[INFO] 10.244.0.70:55942 - 4433 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.013661122s
[INFO] 10.244.0.70:35223 - 19653 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.020353223s
[INFO] 10.244.0.70:54583 - 41489 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000261834s
[INFO] 10.244.0.70:60854 - 62589 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000324482s
[INFO] 10.244.0.70:47057 - 34316 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000191504s
[INFO] 10.244.0.70:45225 - 45635 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000256386s
[INFO] 10.244.0.70:54161 - 21577 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000147644s
[INFO] 10.244.0.70:57462 - 1163 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000219022s
[INFO] 10.244.0.70:51604 - 54867 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.013799906s
[INFO] 10.244.0.70:60638 - 48678 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.020349746s
[INFO] 10.244.0.70:40154 - 33633 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000234038s
[INFO] 10.244.0.70:38527 - 53590 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000245841s
[INFO] 10.244.0.70:57836 - 33814 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.00017586s
[INFO] 10.244.0.70:51819 - 65008 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000221606s
[INFO] 10.244.0.70:39413 - 3153 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000160146s
[INFO] 10.244.0.70:53570 - 36833 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000178583s
[INFO] 10.244.0.70:53580 - 56069 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.013769458s
[INFO] 10.244.0.70:56680 - 17770 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.016510513s
[INFO] 10.244.0.70:42484 - 24630 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000291097s
[INFO] 10.244.0.70:41769 - 11991 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000386918s
[INFO] 10.244.0.70:50394 - 48113 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000173136s
[INFO] 10.244.0.70:57230 - 38241 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000241091s
[INFO] 10.244.0.70:49745 - 43123 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000167129s
[INFO] 10.244.0.70:35088 - 49693 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000206868s
[INFO] 10.244.0.70:54870 - 53872 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.014268284s
[INFO] 10.244.0.70:41854 - 29255 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.016971336s
[INFO] 10.244.0.70:54296 - 44032 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000216088s
[INFO] 10.244.0.70:53726 - 31096 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000256177s
[INFO] 10.244.0.70:34685 - 39362 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.00015826s
[INFO] 10.244.0.70:58989 - 26798 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000270634s
[INFO] 10.244.0.70:47609 - 60700 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000143873s
[INFO] 10.244.0.70:38206 - 413 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00021525s
[INFO] 10.244.0.70:52566 - 49829 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.008566003s
[INFO] 10.244.0.70:58854 - 32371 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.015266901s
[INFO] 10.244.0.70:59244 - 1049 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.00027231s
[INFO] 10.244.0.70:58603 - 65012 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000354443s
[INFO] 10.244.0.70:54794 - 33914 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000228101s
[INFO] 10.244.0.70:36769 - 8934 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000316729s
[INFO] 10.244.0.70:39395 - 64547 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000193251s
[INFO] 10.244.0.70:38604 - 59180 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000286697s
[INFO] 10.244.0.70:39224 - 52919 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.013056302s
[INFO] 10.244.0.70:60130 - 58005 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.01634861s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [b76de3768467] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:43373 - 19542 "HINFO IN 8477786880903177936.5221741515128100926. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.01510929s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.79:58283 - 22649 "A IN api.telegram.org.default.svc.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000414785s
[INFO] 10.244.0.79:54203 - 17616 "AAAA IN api.telegram.org.default.svc.cluster.local. udp 71 false 1232" NXDOMAIN qr,aa,rd 153 0.000501387s
[INFO] 10.244.0.79:45922 - 37283 "A IN api.telegram.org.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.000198907s
[INFO] 10.244.0.79:56509 - 25664 "AAAA IN api.telegram.org.svc.cluster.local. udp 63 false 1232" NXDOMAIN qr,aa,rd 145 0.000251428s
[INFO] 10.244.0.79:41081 - 31848 "A IN api.telegram.org.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000166082s
[INFO] 10.244.0.79:40095 - 46198 "AAAA IN api.telegram.org.cluster.local. udp 59 false 1232" NXDOMAIN qr,aa,rd 141 0.000220837s
[INFO] 10.244.0.79:38147 - 23786 "AAAA IN api.telegram.org. udp 45 false 1232" NOERROR qr,rd,ra 78 0.009304688s
[INFO] 10.244.0.79:54467 - 23850 "A IN api.telegram.org. udp 45 false 1232" NOERROR qr,rd,ra 66 0.016287446s
[INFO] 10.244.0.83:33712 - 38154 "AAAA IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000269446s
[INFO] 10.244.0.83:50205 - 64532 "A IN grafana.com.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000432385s
[INFO] 10.244.0.83:43778 - 6472 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000147365s
[INFO] 10.244.0.83:50956 - 24229 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.00018417s
[INFO] 10.244.0.83:41253 - 17453 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000090723s
[INFO] 10.244.0.83:56730 - 29075 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000077244s
[INFO] 10.244.0.83:40689 - 9156 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 0.008839059s
[INFO] 10.244.0.83:58417 - 30493 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 0.01260418s
[INFO] 10.244.0.83:54677 - 21835 "A IN grafana.net.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000199745s
[INFO] 10.244.0.83:50287 - 32294 "AAAA IN grafana.net.default.svc.cluster.local. udp 66 false 1232" NXDOMAIN qr,aa,rd 148 0.000279712s
[INFO] 10.244.0.83:55811 - 62319 "A IN grafana.net.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000150787s
[INFO] 10.244.0.83:34780 - 19900 "AAAA IN grafana.net.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000189968s
[INFO] 10.244.0.83:48352 - 642 "A IN grafana.net.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.00007438s
[INFO] 10.244.0.83:48135 - 38250 "AAAA IN grafana.net.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000109789s
[INFO] 10.244.0.83:40130 - 22271 "A IN grafana.net. udp 40 false 1232" NOERROR qr,rd,ra 56 0.012368816s
[INFO] 10.244.0.83:51212 - 14513 "AAAA IN grafana.net. udp 40 false 1232" NOERROR qr,rd,ra 125 0.030235436s
[INFO] 10.244.0.87:52859 - 44716 "A IN prometheus-prometheus-pushgateway.default.svc.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000414576s
[INFO] 10.244.0.87:42935 - 39301 "AAAA IN prometheus-prometheus-pushgateway.default.svc.default.svc.cluster.local. udp 100 false 1232" NXDOMAIN qr,aa,rd 182 0.000515775s
[INFO] 10.244.0.87:37789 - 37682 "AAAA IN prometheus-prometheus-pushgateway.default.svc.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.00023697s
[INFO] 10.244.0.87:42037 - 8371 "A IN prometheus-prometheus-pushgateway.default.svc.svc.cluster.local. udp 92 false 1232" NXDOMAIN qr,aa,rd 174 0.000331744s
[INFO] 10.244.0.87:49818 - 38541 "A IN prometheus-prometheus-pushgateway.default.svc.cluster.local. udp 88 false 1232" NOERROR qr,aa,rd 152 0.000236481s
[INFO] 10.244.0.87:57316 - 34976 "AAAA IN prometheus-prometheus-pushgateway.default.svc.cluster.local. udp 88 false 1232" NOERROR qr,aa,rd 170 0.000289979s
[INFO] 10.244.0.89:48772 - 9192 "A IN apache-service.default.svc.cluster.local. udp 58 false 512" NOERROR qr,aa,rd 114 0.00031212s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_04_19T12_54_22_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 19 Apr 2024 09:54:18 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 08 May 2024 07:32:23 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 08 May 2024 07:31:29 +0000   Fri, 19 Apr 2024 09:54:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 08 May 2024 07:31:29 +0000   Fri, 19 Apr 2024 09:54:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 08 May 2024 07:31:29 +0000   Fri, 19 Apr 2024 09:54:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 08 May 2024 07:31:29 +0000   Fri, 19 Apr 2024 09:54:19 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  51288992Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15732696Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  51288992Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             15732696Ki
  pods:               110
System Info:
  Machine ID:                 bbe639a65bf34118ba5f9cd2c1923d15
  System UUID:                6332e0d4-b47c-4a73-9005-7fcd2ce778a8
  Boot ID:                    1c503b43-fcd9-4fe8-bc21-a04a1b695fe4
  Kernel Version:             6.6.9-amd64
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     task31-apache-deployment-55478b4d75-9b7mp     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m3s
  default                     task31-nginx-deployment-744d867495-nt4b8      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m3s
  ingress-nginx               ingress-nginx-controller-7c6974c4d8-6hb77     100m (0%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         18d
  kube-system                 coredns-5dd5756b68-tgx9k                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     18d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         18d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-proxy-5lj79                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 metrics-server-7c66d45ddc-zpqcm               100m (0%!)(MISSING)     0 (0%!)(MISSING)      200Mi (1%!)(MISSING)       0 (0%!)(MISSING)         2d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-759q7    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-fmz9j         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (7%!)(MISSING)   0 (0%!)(MISSING)
  memory             460Mi (2%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 11m                kube-proxy       
  Normal  Starting                 11m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    11m (x8 over 11m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     11m (x7 over 11m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  11m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [May 8 06:56] TSC synchronization [CPU#0 -> CPU#2]:
[  +0.000002] Measured 6224181774 cycles TSC warp between CPUs, turning off TSC clock.
[  +0.000009]   #1  #3  #5  #7  #9 #11
[  +0.190197] pci 0000:00:00.2: can't derive routing for PCI INT A
[  +0.000006] pci 0000:00:00.2: PCI INT A: not connected
[  +0.516560] i8042: PNP: PS/2 appears to have AUX port disabled, if this is incorrect please boot with i8042.nopnp
[  +0.001683] amd_pstate: the _CPC object is not present in SBIOS or ACPI disabled
[  +0.173105] Unstable clock detected, switching default tracing clock to "global"
              If you want to keep using the local clock, then add:
                "trace_clock=local"
              on the kernel command line
[  +1.093071] ACPI Warning: \_SB.PCI0.GPP0.PEGP._DSM: Argument #4 type mismatch - Found [Buffer], ACPI requires [Package] (20230628/nsarguments-61)
[  +2.255538] [drm] psp gfx command LOAD_TA(0x1) failed and response status is (0x7)
[  +0.000112] [drm] psp gfx command INVOKE_CMD(0x3) failed and response status is (0x4)
[  +0.000002] amdgpu 0000:05:00.0: amdgpu: Secure display: Generic Failure.
[  +0.000007] amdgpu 0000:05:00.0: amdgpu: SECUREDISPLAY: query securedisplay TA failed. ret 0x0
[  +0.889342] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.556492] alsactl[743]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.952118] warning: `atop' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[  +2.751810] snd_hda_codec_hdmi hdaudioC0D0: HDMI: pin NID 0x5 not registered

* 
* ==> etcd [b1f62ac02f22] <==
* {"level":"info","ts":"2024-05-07T08:34:46.372205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-05-07T08:34:46.376345Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-07T08:34:46.376372Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-07T08:34:46.376386Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-07T08:34:46.376765Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-07T08:34:46.376787Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-07T08:34:46.377343Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-07T08:34:46.378287Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-07T08:44:46.401398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24141}
{"level":"info","ts":"2024-05-07T08:44:46.418083Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24141,"took":"16.281576ms","hash":96038126}
{"level":"info","ts":"2024-05-07T08:44:46.418171Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":96038126,"revision":24141,"compact-revision":23106}
{"level":"info","ts":"2024-05-07T08:49:46.409311Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24421}
{"level":"info","ts":"2024-05-07T08:49:46.411032Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24421,"took":"1.340109ms","hash":3929258181}
{"level":"info","ts":"2024-05-07T08:49:46.411082Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3929258181,"revision":24421,"compact-revision":24141}
{"level":"info","ts":"2024-05-07T08:53:37.86696Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":30003,"local-member-snapshot-index":20002,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-05-07T08:53:37.871532Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2024-05-07T08:53:37.871657Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2024-05-07T08:54:46.415306Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24702}
{"level":"info","ts":"2024-05-07T08:54:46.41707Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24702,"took":"1.39326ms","hash":3781020980}
{"level":"info","ts":"2024-05-07T08:54:46.417115Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3781020980,"revision":24702,"compact-revision":24421}
{"level":"info","ts":"2024-05-07T08:59:46.423428Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24982}
{"level":"info","ts":"2024-05-07T08:59:46.425116Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24982,"took":"1.260423ms","hash":3316822175}
{"level":"info","ts":"2024-05-07T08:59:46.425167Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3316822175,"revision":24982,"compact-revision":24702}
{"level":"info","ts":"2024-05-07T09:04:46.42926Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25261}
{"level":"info","ts":"2024-05-07T09:04:46.43099Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":25261,"took":"1.317762ms","hash":496467478}
{"level":"info","ts":"2024-05-07T09:04:46.431039Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":496467478,"revision":25261,"compact-revision":24982}
{"level":"info","ts":"2024-05-07T09:09:46.437476Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25542}
{"level":"info","ts":"2024-05-07T09:09:46.439201Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":25542,"took":"1.338645ms","hash":3325027249}
{"level":"info","ts":"2024-05-07T09:09:46.43925Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3325027249,"revision":25542,"compact-revision":25261}
{"level":"info","ts":"2024-05-07T09:14:46.444693Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25820}
{"level":"info","ts":"2024-05-07T09:14:46.446419Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":25820,"took":"1.307496ms","hash":2487966602}
{"level":"info","ts":"2024-05-07T09:14:46.446469Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2487966602,"revision":25820,"compact-revision":25542}
{"level":"info","ts":"2024-05-07T09:19:46.44998Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26101}
{"level":"info","ts":"2024-05-07T09:19:46.451683Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":26101,"took":"1.286892ms","hash":2128878800}
{"level":"info","ts":"2024-05-07T09:19:46.451738Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2128878800,"revision":26101,"compact-revision":25820}
{"level":"info","ts":"2024-05-07T09:24:46.456961Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26381}
{"level":"info","ts":"2024-05-07T09:24:46.458467Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":26381,"took":"1.188412ms","hash":1471893942}
{"level":"info","ts":"2024-05-07T09:24:46.458513Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1471893942,"revision":26381,"compact-revision":26101}
{"level":"info","ts":"2024-05-07T09:29:46.464427Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26661}
{"level":"info","ts":"2024-05-07T09:29:46.468302Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":26661,"took":"2.737138ms","hash":3604772740}
{"level":"info","ts":"2024-05-07T09:29:46.468384Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3604772740,"revision":26661,"compact-revision":26381}
{"level":"info","ts":"2024-05-07T09:34:46.473291Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26941}
{"level":"info","ts":"2024-05-07T09:34:46.474976Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":26941,"took":"1.287869ms","hash":3380643968}
{"level":"info","ts":"2024-05-07T09:34:46.475026Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3380643968,"revision":26941,"compact-revision":26661}
{"level":"info","ts":"2024-05-07T09:39:46.480426Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27220}
{"level":"info","ts":"2024-05-07T09:39:46.481764Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27220,"took":"1.053134ms","hash":1301674753}
{"level":"info","ts":"2024-05-07T09:39:46.481801Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1301674753,"revision":27220,"compact-revision":26941}
{"level":"info","ts":"2024-05-07T09:44:46.488092Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27513}
{"level":"info","ts":"2024-05-07T09:44:46.502678Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27513,"took":"14.122845ms","hash":3652538268}
{"level":"info","ts":"2024-05-07T09:44:46.502735Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3652538268,"revision":27513,"compact-revision":27220}
{"level":"info","ts":"2024-05-07T09:48:51.542506Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-05-07T09:48:51.542563Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-05-07T09:48:51.542637Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T09:48:51.542731Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T09:48:51.577453Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-07T09:48:51.577498Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-05-07T09:48:51.579247Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-07T09:48:51.58385Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-07T09:48:51.583964Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-07T09:48:51.583987Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [ca04ccf08bf0] <==
* {"level":"info","ts":"2024-05-08T07:21:27.027664Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-05-08T07:21:27.028642Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-08T07:21:27.030339Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":27513}
{"level":"info","ts":"2024-05-08T07:21:27.032946Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":28023}
{"level":"info","ts":"2024-05-08T07:21:27.034078Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-08T07:21:27.035384Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-08T07:21:27.035747Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-08T07:21:27.035786Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-08T07:21:27.03589Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-05-08T07:21:27.036284Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-08T07:21:27.036347Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-08T07:21:27.036362Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-08T07:21:27.037693Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-08T07:21:27.037788Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-08T07:21:27.037861Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-08T07:21:27.0379Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-08T07:21:27.037921Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-08T07:21:28.028931Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 9"}
{"level":"info","ts":"2024-05-08T07:21:28.029053Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 9"}
{"level":"info","ts":"2024-05-08T07:21:28.029099Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-05-08T07:21:28.029129Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 10"}
{"level":"info","ts":"2024-05-08T07:21:28.029149Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 10"}
{"level":"info","ts":"2024-05-08T07:21:28.02917Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 10"}
{"level":"info","ts":"2024-05-08T07:21:28.029192Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 10"}
{"level":"info","ts":"2024-05-08T07:21:28.033654Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-08T07:21:28.033682Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-08T07:21:28.03366Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-08T07:21:28.0341Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-08T07:21:28.034285Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-08T07:21:28.034747Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-08T07:21:28.035213Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-08T07:30:55.182237Z","caller":"traceutil/trace.go:171","msg":"trace[1470006735] transaction","detail":"{read_only:false; response_revision:29230; number_of_response:1; }","duration":"377.438973ms","start":"2024-05-08T07:30:54.804771Z","end":"2024-05-08T07:30:55.18221Z","steps":["trace[1470006735] 'process raft request'  (duration: 377.283995ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T07:30:55.182943Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:54.804709Z","time spent":"377.606033ms","remote":"127.0.0.1:46148","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:29229 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-08T07:30:56.915112Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"178.504908ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-05-08T07:30:56.915201Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.857816ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" ","response":"range_response_count:1 size:503"}
{"level":"info","ts":"2024-05-08T07:30:56.915253Z","caller":"traceutil/trace.go:171","msg":"trace[355656906] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29231; }","duration":"178.664564ms","start":"2024-05-08T07:30:56.736563Z","end":"2024-05-08T07:30:56.915227Z","steps":["trace[355656906] 'range keys from in-memory index tree'  (duration: 178.325905ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T07:30:56.915291Z","caller":"traceutil/trace.go:171","msg":"trace[1035455728] range","detail":"{range_begin:/registry/leases/ingress-nginx/ingress-nginx-leader; range_end:; response_count:1; response_revision:29231; }","duration":"159.951404ms","start":"2024-05-08T07:30:56.755317Z","end":"2024-05-08T07:30:56.915269Z","steps":["trace[1035455728] 'range keys from in-memory index tree'  (duration: 159.631112ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T07:30:56.915291Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"284.956193ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-08T07:30:56.915386Z","caller":"traceutil/trace.go:171","msg":"trace[457959570] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:29231; }","duration":"285.065076ms","start":"2024-05-08T07:30:56.630294Z","end":"2024-05-08T07:30:56.915359Z","steps":["trace[457959570] 'count revisions from in-memory index tree'  (duration: 284.781451ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T07:30:57.562352Z","caller":"traceutil/trace.go:171","msg":"trace[1303371739] linearizableReadLoop","detail":"{readStateIndex:35142; appliedIndex:35141; }","duration":"375.988025ms","start":"2024-05-08T07:30:57.186337Z","end":"2024-05-08T07:30:57.562325Z","steps":["trace[1303371739] 'read index received'  (duration: 375.724934ms)","trace[1303371739] 'applied index is now lower than readState.Index'  (duration: 256.456¬µs)"],"step_count":2}
{"level":"info","ts":"2024-05-08T07:30:57.562413Z","caller":"traceutil/trace.go:171","msg":"trace[945379383] transaction","detail":"{read_only:false; response_revision:29232; number_of_response:1; }","duration":"642.745158ms","start":"2024-05-08T07:30:56.919627Z","end":"2024-05-08T07:30:57.562372Z","steps":["trace[945379383] 'process raft request'  (duration: 642.522714ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T07:30:57.562552Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:56.919604Z","time spent":"642.85844ms","remote":"127.0.0.1:46258","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":486,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" mod_revision:29224 > success:<request_put:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" value_size:427 >> failure:<request_range:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" > >"}
{"level":"warn","ts":"2024-05-08T07:30:57.562573Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"239.416218ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-05-08T07:30:57.562583Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"376.266761ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-05-08T07:30:57.562634Z","caller":"traceutil/trace.go:171","msg":"trace[745511608] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:29232; }","duration":"239.486199ms","start":"2024-05-08T07:30:57.323128Z","end":"2024-05-08T07:30:57.562614Z","steps":["trace[745511608] 'agreement among raft nodes before linearized reading'  (duration: 239.376758ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T07:30:57.562659Z","caller":"traceutil/trace.go:171","msg":"trace[1323052912] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:29232; }","duration":"376.351897ms","start":"2024-05-08T07:30:57.186282Z","end":"2024-05-08T07:30:57.562634Z","steps":["trace[1323052912] 'agreement among raft nodes before linearized reading'  (duration: 376.180856ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T07:30:57.562711Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:57.18626Z","time spent":"376.433891ms","remote":"127.0.0.1:46148","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-05-08T07:30:58.230634Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"468.346629ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128029031659274983 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:29230 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2024-05-08T07:30:58.230935Z","caller":"traceutil/trace.go:171","msg":"trace[1599637120] transaction","detail":"{read_only:false; response_revision:29233; number_of_response:1; }","duration":"664.647964ms","start":"2024-05-08T07:30:57.566254Z","end":"2024-05-08T07:30:58.230902Z","steps":["trace[1599637120] 'process raft request'  (duration: 195.828929ms)","trace[1599637120] 'compare'  (duration: 468.241588ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-08T07:30:58.231013Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:57.566238Z","time spent":"664.73331ms","remote":"127.0.0.1:46148","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:29230 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-05-08T07:30:58.231125Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"494.000182ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-05-08T07:30:58.231202Z","caller":"traceutil/trace.go:171","msg":"trace[258444265] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:29233; }","duration":"494.131135ms","start":"2024-05-08T07:30:57.737048Z","end":"2024-05-08T07:30:58.231179Z","steps":["trace[258444265] 'agreement among raft nodes before linearized reading'  (duration: 493.877402ms)"],"step_count":1}
{"level":"info","ts":"2024-05-08T07:30:58.231174Z","caller":"traceutil/trace.go:171","msg":"trace[427296582] linearizableReadLoop","detail":"{readStateIndex:35143; appliedIndex:35142; }","duration":"493.961631ms","start":"2024-05-08T07:30:57.737123Z","end":"2024-05-08T07:30:58.231084Z","steps":["trace[427296582] 'read index received'  (duration: 24.906673ms)","trace[427296582] 'applied index is now lower than readState.Index'  (duration: 468.875885ms)"],"step_count":2}
{"level":"warn","ts":"2024-05-08T07:30:58.231259Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:57.737024Z","time spent":"494.21222ms","remote":"127.0.0.1:45966","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-05-08T07:30:58.803759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"395.954976ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-05-08T07:30:58.803853Z","caller":"traceutil/trace.go:171","msg":"trace[1903809918] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:29233; }","duration":"396.071051ms","start":"2024-05-08T07:30:58.407758Z","end":"2024-05-08T07:30:58.803829Z","steps":["trace[1903809918] 'count revisions from in-memory index tree'  (duration: 395.791966ms)"],"step_count":1}
{"level":"warn","ts":"2024-05-08T07:30:58.803911Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-05-08T07:30:58.407737Z","time spent":"396.154651ms","remote":"127.0.0.1:46106","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":6,"response size":32,"request content":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true "}
{"level":"info","ts":"2024-05-08T07:31:28.067301Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28937}
{"level":"info","ts":"2024-05-08T07:31:28.082633Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":28937,"took":"15.086173ms","hash":498908907}
{"level":"info","ts":"2024-05-08T07:31:28.082696Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":498908907,"revision":28937,"compact-revision":27513}

* 
* ==> kernel <==
*  07:32:33 up 36 min,  0 users,  load average: 1.18, 1.26, 0.96
Linux minikube 6.6.9-amd64 #1 SMP PREEMPT_DYNAMIC Kali 6.6.9-1kali1 (2024-01-08) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [81659abc60ce] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.574860       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.573844       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.574864       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.574896       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.575027       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.575057       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0507 09:48:51.575085       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [b89fbdcb6624] <==
* I0508 07:21:48.647203       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0508 07:21:49.669707       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0508 07:21:49.669736       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0508 07:21:49.670681       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:21:49.670786       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0508 07:21:49.671468       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:21:49.671996       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0508 07:21:50.670753       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:21:50.670777       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0508 07:21:50.670791       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0508 07:21:50.670990       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:21:50.671027       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0508 07:21:50.671867       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0508 07:22:02.696741       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:22:02.696800       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0508 07:22:02.696960       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1: Get "https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.96.66.41:443: connect: connection refused
I0508 07:22:02.697300       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0508 07:22:02.699000       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1: Get "https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1": dial tcp 10.96.66.41:443: connect: connection refused
W0508 07:22:03.697517       1 handler_proxy.go:93] no RequestInfo found in the context
W0508 07:22:03.697560       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:22:03.697563       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I0508 07:22:03.697589       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0508 07:22:03.697639       1 controller.go:102] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I0508 07:22:03.698614       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0508 07:22:07.710625       1 handler_proxy.go:93] no RequestInfo found in the context
E0508 07:22:07.710711       1 controller.go:146] Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
E0508 07:22:07.710919       1 available_controller.go:460] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1: Get "https://10.96.66.41:443/apis/metrics.k8s.io/v1beta1": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
I0508 07:22:07.726953       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:22:07.747642       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:22:29.164383       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:23:29.165138       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:23:40.583526       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0508 07:23:40.592660       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0508 07:24:29.164702       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:25:29.164913       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:26:29.165512       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:26:29.196906       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:27:29.164552       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:28:29.165466       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:29:29.164601       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:30:29.165062       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:30:30.901233       1 alloc.go:330] "allocated clusterIPs" service="default/task31-nginx-service" clusterIPs={"IPv4":"10.109.16.12"}
I0508 07:30:30.906830       1 alloc.go:330] "allocated clusterIPs" service="default/apache-service" clusterIPs={"IPv4":"10.111.170.7"}
I0508 07:30:30.915707       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0508 07:30:57.563374       1 trace.go:236] Trace[659553625]: "Update" accept:application/json, */*,audit-id:dda8322f-6e00-4284-b914-aecb7da3beee,client:10.244.0.76,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/ingress-nginx/leases/ingress-nginx-leader,user-agent:nginx-ingress-controller/v1.9.4 (linux/amd64) ingress-nginx/846d251814a09d8a5d8d28e2e604bfc7749bcb49,verb:PUT (08-May-2024 07:30:56.918) (total time: 645ms):
Trace[659553625]: ["GuaranteedUpdate etcd3" audit-id:dda8322f-6e00-4284-b914-aecb7da3beee,key:/leases/ingress-nginx/ingress-nginx-leader,type:*coordination.Lease,resource:leases.coordination.k8s.io 645ms (07:30:56.918)
Trace[659553625]:  ---"Txn call completed" 644ms (07:30:57.563)]
Trace[659553625]: [645.310073ms] [645.310073ms] END
I0508 07:30:58.231731       1 trace.go:236] Trace[1390039291]: "Update" accept:application/json, */*,audit-id:f79994c6-d1c2-43c0-84cf-b3c9b1623946,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (08-May-2024 07:30:57.564) (total time: 666ms):
Trace[1390039291]: ["GuaranteedUpdate etcd3" audit-id:f79994c6-d1c2-43c0-84cf-b3c9b1623946,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 666ms (07:30:57.565)
Trace[1390039291]:  ---"Txn call completed" 665ms (07:30:58.231)]
Trace[1390039291]: [666.920665ms] [666.920665ms] END
I0508 07:31:29.165378       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:31:29.201323       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0508 07:32:29.165250       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-controller-manager [80c316c4bb21] <==
* I0507 08:34:59.927294       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0507 08:34:59.928806       1 shared_informer.go:318] Caches are synced for expand
I0507 08:34:59.932618       1 shared_informer.go:318] Caches are synced for PV protection
I0507 08:34:59.934894       1 shared_informer.go:318] Caches are synced for ephemeral
I0507 08:34:59.938266       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0507 08:34:59.940511       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0507 08:34:59.941711       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0507 08:34:59.942863       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0507 08:34:59.947289       1 shared_informer.go:318] Caches are synced for disruption
I0507 08:34:59.948548       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0507 08:34:59.948613       1 shared_informer.go:318] Caches are synced for crt configmap
I0507 08:34:59.948794       1 shared_informer.go:318] Caches are synced for daemon sets
I0507 08:34:59.949570       1 shared_informer.go:318] Caches are synced for service account
I0507 08:34:59.952185       1 shared_informer.go:318] Caches are synced for persistent volume
I0507 08:34:59.952960       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0507 08:34:59.954828       1 shared_informer.go:318] Caches are synced for PVC protection
I0507 08:34:59.956013       1 shared_informer.go:318] Caches are synced for TTL after finished
I0507 08:34:59.958742       1 shared_informer.go:318] Caches are synced for attach detach
I0507 08:34:59.958911       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="73.975214ms"
I0507 08:34:59.959045       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="74.147372ms"
I0507 08:34:59.959100       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="29.543¬µs"
I0507 08:34:59.959141       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="20.393¬µs"
I0507 08:34:59.959238       1 shared_informer.go:318] Caches are synced for endpoint
I0507 08:34:59.959234       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="74.280699ms"
I0507 08:34:59.959252       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="74.472831ms"
I0507 08:34:59.959383       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="66.349¬µs"
I0507 08:34:59.959437       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="40.089¬µs"
I0507 08:34:59.959541       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="37.015¬µs"
I0507 08:34:59.959660       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="91.632¬µs"
I0507 08:34:59.960988       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="76.118637ms"
I0507 08:34:59.961132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="78.711¬µs"
I0507 08:34:59.965151       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="6.186874ms"
I0507 08:34:59.965255       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="55.314¬µs"
I0507 08:35:00.108901       1 shared_informer.go:318] Caches are synced for resource quota
I0507 08:35:00.152506       1 shared_informer.go:318] Caches are synced for resource quota
I0507 08:35:00.469409       1 shared_informer.go:318] Caches are synced for garbage collector
I0507 08:35:00.480856       1 shared_informer.go:318] Caches are synced for garbage collector
I0507 08:35:00.480887       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0507 08:35:04.100467       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="6.82627ms"
I0507 08:35:04.100576       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="63.486¬µs"
I0507 08:35:14.875755       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
E0507 08:35:14.884113       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/php-apache: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)
I0507 08:35:14.884236       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server is currently unable to handle the request (get pods.metrics.k8s.io)"
I0507 08:35:19.144049       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="7.70124ms"
I0507 08:35:19.144134       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="50.146¬µs"
I0507 08:35:19.388373       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="7.093902ms"
I0507 08:35:19.388582       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="90.794¬µs"
I0507 08:35:19.405948       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="78.012¬µs"
I0507 08:35:19.429342       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="84.508¬µs"
I0507 08:35:20.473046       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="8.171969ms"
I0507 08:35:20.473188       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="67.326¬µs"
I0507 08:35:21.330726       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="8.180839ms"
I0507 08:35:21.331000       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="61.46¬µs"
I0507 08:35:21.921061       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="73.403¬µs"
I0507 08:35:25.900973       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="8.41739ms"
I0507 08:35:25.901046       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="37.784¬µs"
I0507 08:35:32.022948       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="12.542276ms"
I0507 08:35:32.023156       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-7c6974c4d8" duration="160.564¬µs"
I0507 08:35:35.869335       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="13.162745ms"
I0507 08:35:35.869464       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="63.835¬µs"

* 
* ==> kube-controller-manager [a75c46f7d1af] <==
* I0508 07:22:16.588342       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="78.641¬µs"
I0508 07:22:16.591800       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="75.638¬µs"
I0508 07:22:16.601608       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="96.939¬µs"
I0508 07:22:16.623301       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="77.663¬µs"
I0508 07:22:16.630055       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="52.94¬µs"
I0508 07:22:16.633096       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="59.575¬µs"
I0508 07:22:16.641509       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/grafana-55dc9f456" duration="76.336¬µs"
I0508 07:22:17.681617       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/php-apache-598b474864" duration="5.431873ms"
I0508 07:22:17.681715       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/php-apache-598b474864" duration="49.517¬µs"
I0508 07:22:18.705628       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="5.872639ms"
I0508 07:22:18.705722       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="47.771¬µs"
I0508 07:22:24.691803       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/grafana-55dc9f456" duration="12.50382ms"
I0508 07:22:24.691955       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/grafana-55dc9f456" duration="86.184¬µs"
I0508 07:22:24.706811       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="10.480879ms"
I0508 07:22:24.706984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="110.698¬µs"
I0508 07:22:26.699503       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
E0508 07:22:26.708405       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/php-apache: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I0508 07:22:26.708485       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0508 07:22:34.741686       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="8.45787ms"
I0508 07:22:34.741885       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="59.225¬µs"
E0508 07:22:41.726431       1 horizontal.go:274] failed to compute desired number of replicas based on listed metrics for Deployment/default/php-apache: invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
I0508 07:22:41.726521       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetResourceMetric" message="failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0508 07:22:41.726555       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedComputeMetricsReplicas" message="invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API"
I0508 07:22:49.759003       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="10.174003ms"
I0508 07:22:49.759176       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="106.228¬µs"
I0508 07:22:53.065784       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/grafana-55dc9f456" duration="7.194¬µs"
I0508 07:22:53.069628       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/php-apache-598b474864" duration="11.873¬µs"
I0508 07:22:53.076215       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-kube-state-metrics-6b7d7b9bd9" duration="11.803¬µs"
I0508 07:22:53.085530       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-prometheus-pushgateway-568fbf799" duration="16.622¬µs"
I0508 07:22:53.093675       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/prometheus-server-579dc9cfdf" duration="5.657¬µs"
I0508 07:22:56.731547       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
E0508 07:22:56.740578       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
E0508 07:23:11.751216       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
I0508 07:23:11.751279       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
E0508 07:23:26.755836       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
I0508 07:23:26.755891       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
E0508 07:23:41.760310       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
I0508 07:23:41.760341       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
E0508 07:23:56.765646       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
I0508 07:23:56.765681       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
E0508 07:24:11.774619       1 horizontal.go:274] failed to query scale subresource for Deployment/default/php-apache: deployments/scale.apps "php-apache" not found
I0508 07:24:11.774736       1 event.go:307] "Event occurred" object="default/php-apache" fieldPath="" kind="HorizontalPodAutoscaler" apiVersion="autoscaling/v2" type="Warning" reason="FailedGetScale" message="deployments/scale.apps \"php-apache\" not found"
I0508 07:24:26.775544       1 horizontal.go:512] "Horizontal Pod Autoscaler has been deleted" HPA="default/php-apache"
I0508 07:24:36.776027       1 stateful_set.go:458] "StatefulSet has been deleted" key="default/prometheus-alertmanager"
I0508 07:30:30.918298       1 event.go:307] "Event occurred" object="default/task31-apache-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set task31-apache-deployment-55478b4d75 to 1"
I0508 07:30:30.920096       1 event.go:307] "Event occurred" object="default/task31-nginx-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set task31-nginx-deployment-744d867495 to 1"
I0508 07:30:30.927229       1 event.go:307] "Event occurred" object="default/task31-apache-deployment-55478b4d75" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: task31-apache-deployment-55478b4d75-9b7mp"
I0508 07:30:30.928186       1 event.go:307] "Event occurred" object="default/task31-nginx-deployment-744d867495" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: task31-nginx-deployment-744d867495-nt4b8"
I0508 07:30:30.942759       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="24.762659ms"
I0508 07:30:30.945238       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="25.215089ms"
I0508 07:30:30.955884       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="10.052163ms"
I0508 07:30:30.955907       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="12.876398ms"
I0508 07:30:30.955964       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="32.756¬µs"
I0508 07:30:30.956019       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="26.261¬µs"
I0508 07:30:30.964352       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="89.676¬µs"
I0508 07:30:30.976189       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="39.53¬µs"
I0508 07:30:40.495886       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="6.519389ms"
I0508 07:30:40.496006       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-nginx-deployment-744d867495" duration="59.644¬µs"
I0508 07:30:59.791658       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="12.376755ms"
I0508 07:30:59.791797       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/task31-apache-deployment-55478b4d75" duration="65.371¬µs"

* 
* ==> kube-proxy [21155605e93e] <==
* I0508 07:21:30.868059       1 server_others.go:69] "Using iptables proxy"
I0508 07:21:30.903362       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0508 07:21:30.956429       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0508 07:21:30.958657       1 server_others.go:152] "Using iptables Proxier"
I0508 07:21:30.958689       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0508 07:21:30.958700       1 server_others.go:438] "Defaulting to no-op detect-local"
I0508 07:21:30.960153       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0508 07:21:30.960376       1 server.go:846] "Version info" version="v1.28.3"
I0508 07:21:30.960387       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0508 07:21:30.963104       1 config.go:97] "Starting endpoint slice config controller"
I0508 07:21:30.963196       1 config.go:188] "Starting service config controller"
I0508 07:21:30.963381       1 config.go:315] "Starting node config controller"
I0508 07:21:30.963527       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0508 07:21:30.963638       1 shared_informer.go:311] Waiting for caches to sync for node config
I0508 07:21:30.963647       1 shared_informer.go:311] Waiting for caches to sync for service config
I0508 07:21:31.064540       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0508 07:21:31.064689       1 shared_informer.go:318] Caches are synced for service config
I0508 07:21:31.064709       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [f819b1370b8e] <==
* I0507 08:34:48.859079       1 server_others.go:69] "Using iptables proxy"
I0507 08:34:48.891138       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0507 08:34:48.929481       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0507 08:34:48.931593       1 server_others.go:152] "Using iptables Proxier"
I0507 08:34:48.931617       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0507 08:34:48.931625       1 server_others.go:438] "Defaulting to no-op detect-local"
I0507 08:34:48.932391       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0507 08:34:48.933641       1 server.go:846] "Version info" version="v1.28.3"
I0507 08:34:48.933685       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0507 08:34:48.937047       1 config.go:315] "Starting node config controller"
I0507 08:34:48.937114       1 config.go:188] "Starting service config controller"
I0507 08:34:48.937047       1 config.go:97] "Starting endpoint slice config controller"
I0507 08:34:48.938602       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0507 08:34:48.938676       1 shared_informer.go:311] Waiting for caches to sync for node config
I0507 08:34:48.938610       1 shared_informer.go:311] Waiting for caches to sync for service config
I0507 08:34:49.038901       1 shared_informer.go:318] Caches are synced for service config
I0507 08:34:49.038910       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0507 08:34:49.039024       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [75f7d6f9cfc4] <==
* I0508 07:21:27.033750       1 serving.go:348] Generated self-signed cert in-memory
W0508 07:21:29.186882       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0508 07:21:29.186953       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found]
W0508 07:21:29.186980       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0508 07:21:29.186995       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0508 07:21:29.225585       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0508 07:21:29.225602       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0508 07:21:29.227082       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0508 07:21:29.227622       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0508 07:21:29.227775       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0508 07:21:29.227920       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0508 07:21:29.328819       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [8e9f88fb8334] <==
* I0507 08:34:45.994585       1 serving.go:348] Generated self-signed cert in-memory
W0507 08:34:47.314716       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0507 08:34:47.314761       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0507 08:34:47.314774       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0507 08:34:47.314783       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0507 08:34:47.328055       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0507 08:34:47.328074       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0507 08:34:47.329625       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0507 08:34:47.329652       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0507 08:34:47.330035       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0507 08:34:47.330070       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0507 08:34:47.430681       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0507 09:48:51.540259       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0507 09:48:51.540380       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0507 09:48:51.540733       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0507 09:48:51.542063       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* May 08 07:24:37 minikube kubelet[1568]: I0508 07:24:37.320967    1568 reconciler_common.go:300] "Volume detached for volume \"pvc-eaf8ff80-17a6-40e4-b1e3-63d2ec42bc63\" (UniqueName: \"kubernetes.io/host-path/f1ff8fb1-4521-4a01-8b87-e8fd04e58abe-pvc-eaf8ff80-17a6-40e4-b1e3-63d2ec42bc63\") on node \"minikube\" DevicePath \"\""
May 08 07:24:37 minikube kubelet[1568]: I0508 07:24:37.470028    1568 scope.go:117] "RemoveContainer" containerID="9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6"
May 08 07:24:37 minikube kubelet[1568]: I0508 07:24:37.495372    1568 scope.go:117] "RemoveContainer" containerID="9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6"
May 08 07:24:37 minikube kubelet[1568]: E0508 07:24:37.496410    1568 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6" containerID="9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6"
May 08 07:24:37 minikube kubelet[1568]: I0508 07:24:37.496481    1568 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6"} err="failed to get container status \"9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6\": rpc error: code = Unknown desc = Error response from daemon: No such container: 9b39a012bb8799243f08212fcdbd378be8bb8a5b688d4f435713018528cd92e6"
May 08 07:24:39 minikube kubelet[1568]: I0508 07:24:39.504327    1568 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="f1ff8fb1-4521-4a01-8b87-e8fd04e58abe" path="/var/lib/kubelet/pods/f1ff8fb1-4521-4a01-8b87-e8fd04e58abe/volumes"
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002113    1568 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-proc\") pod \"3d768131-10d6-44fe-a409-a84b2597ab4f\" (UID: \"3d768131-10d6-44fe-a409-a84b2597ab4f\") "
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002199    1568 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"root\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-root\") pod \"3d768131-10d6-44fe-a409-a84b2597ab4f\" (UID: \"3d768131-10d6-44fe-a409-a84b2597ab4f\") "
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002254    1568 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"sys\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-sys\") pod \"3d768131-10d6-44fe-a409-a84b2597ab4f\" (UID: \"3d768131-10d6-44fe-a409-a84b2597ab4f\") "
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002273    1568 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-proc" (OuterVolumeSpecName: "proc") pod "3d768131-10d6-44fe-a409-a84b2597ab4f" (UID: "3d768131-10d6-44fe-a409-a84b2597ab4f"). InnerVolumeSpecName "proc". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002355    1568 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-root" (OuterVolumeSpecName: "root") pod "3d768131-10d6-44fe-a409-a84b2597ab4f" (UID: "3d768131-10d6-44fe-a409-a84b2597ab4f"). InnerVolumeSpecName "root". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.002376    1568 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-sys" (OuterVolumeSpecName: "sys") pod "3d768131-10d6-44fe-a409-a84b2597ab4f" (UID: "3d768131-10d6-44fe-a409-a84b2597ab4f"). InnerVolumeSpecName "sys". PluginName "kubernetes.io/host-path", VolumeGidValue ""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.103014    1568 reconciler_common.go:300] "Volume detached for volume \"proc\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-proc\") on node \"minikube\" DevicePath \"\""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.103077    1568 reconciler_common.go:300] "Volume detached for volume \"root\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-root\") on node \"minikube\" DevicePath \"\""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.103108    1568 reconciler_common.go:300] "Volume detached for volume \"sys\" (UniqueName: \"kubernetes.io/host-path/3d768131-10d6-44fe-a409-a84b2597ab4f-sys\") on node \"minikube\" DevicePath \"\""
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.641981    1568 scope.go:117] "RemoveContainer" containerID="9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b"
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.665212    1568 scope.go:117] "RemoveContainer" containerID="9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b"
May 08 07:24:48 minikube kubelet[1568]: E0508 07:24:48.666401    1568 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b" containerID="9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b"
May 08 07:24:48 minikube kubelet[1568]: I0508 07:24:48.666491    1568 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b"} err="failed to get container status \"9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b\": rpc error: code = Unknown desc = Error response from daemon: No such container: 9228551d87a069b6a05ac148d29612ebf4bac60b6e5134d771e372ab9b4a8c2b"
May 08 07:24:49 minikube kubelet[1568]: I0508 07:24:49.504265    1568 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="3d768131-10d6-44fe-a409-a84b2597ab4f" path="/var/lib/kubelet/pods/3d768131-10d6-44fe-a409-a84b2597ab4f/volumes"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.933635    1568 topology_manager.go:215] "Topology Admit Handler" podUID="c7f566f6-d6c5-4b3b-ae7b-efcfadf50931" podNamespace="default" podName="task31-nginx-deployment-744d867495-nt4b8"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933756    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="bbd1b922-718c-4a6f-bcb9-5f1ae0620671" containerName="grafana"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933779    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0969978f-a5c9-4047-857d-a01b492205a3" containerName="alertmanager"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933794    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f1ff8fb1-4521-4a01-8b87-e8fd04e58abe" containerName="alertmanager"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933807    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="fe661cf8-3e65-4dcd-b1f3-01bf92700c91" containerName="pushgateway"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933823    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="29fce4b3-e163-4668-a885-dcafe3f8d9e5" containerName="php-apache"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933834    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="22b8b2d0-bcc5-4704-8f67-6622b0d49b35" containerName="grafana"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933847    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="5e7b7637-4158-4663-b4c0-8a9697100bc2" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933858    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="caf1391a-da6f-4684-8cee-0733f51da5e3" containerName="prometheus-server-configmap-reload"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933869    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="cfecb135-7dc0-4a13-aad2-613384478127" containerName="pushgateway"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933881    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b66a2f24-d2bf-47bb-89ef-965e0e8e39a1" containerName="prometheus-server-configmap-reload"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933893    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f6f4793b-ad89-4e61-af91-00ae03e97bed" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933905    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b66a2f24-d2bf-47bb-89ef-965e0e8e39a1" containerName="prometheus-server"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933917    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3d768131-10d6-44fe-a409-a84b2597ab4f" containerName="node-exporter"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933928    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933938    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="caf1391a-da6f-4684-8cee-0733f51da5e3" containerName="prometheus-server"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933951    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.933962    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934006    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="3d768131-10d6-44fe-a409-a84b2597ab4f" containerName="node-exporter"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934019    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="bbd1b922-718c-4a6f-bcb9-5f1ae0620671" containerName="grafana"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934030    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="29fce4b3-e163-4668-a885-dcafe3f8d9e5" containerName="php-apache"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934041    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934053    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934062    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="f1ff8fb1-4521-4a01-8b87-e8fd04e58abe" containerName="alertmanager"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934075    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="b66a2f24-d2bf-47bb-89ef-965e0e8e39a1" containerName="prometheus-server-configmap-reload"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934094    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="caf1391a-da6f-4684-8cee-0733f51da5e3" containerName="prometheus-server-configmap-reload"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934105    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="5e7b7637-4158-4663-b4c0-8a9697100bc2" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934117    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="5e7b7637-4158-4663-b4c0-8a9697100bc2" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934129    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="3254c506-1163-4289-9a5f-65073b3389e9" containerName="php-apache"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934140    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="b66a2f24-d2bf-47bb-89ef-965e0e8e39a1" containerName="prometheus-server"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934150    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="fe661cf8-3e65-4dcd-b1f3-01bf92700c91" containerName="pushgateway"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934162    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="caf1391a-da6f-4684-8cee-0733f51da5e3" containerName="prometheus-server"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.934176    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="f6f4793b-ad89-4e61-af91-00ae03e97bed" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.936837    1568 topology_manager.go:215] "Topology Admit Handler" podUID="b3c15d86-ec97-442c-b5b3-32e669de6e64" podNamespace="default" podName="task31-apache-deployment-55478b4d75-9b7mp"
May 08 07:30:30 minikube kubelet[1568]: E0508 07:30:30.939949    1568 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="5e7b7637-4158-4663-b4c0-8a9697100bc2" containerName="kube-state-metrics"
May 08 07:30:30 minikube kubelet[1568]: I0508 07:30:30.940040    1568 memory_manager.go:346] "RemoveStaleState removing state" podUID="a8bf2ddd-efa2-4335-a5a4-ebf8302cd6f1" containerName="load-generator"
May 08 07:30:31 minikube kubelet[1568]: I0508 07:30:31.052704    1568 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-scplh\" (UniqueName: \"kubernetes.io/projected/b3c15d86-ec97-442c-b5b3-32e669de6e64-kube-api-access-scplh\") pod \"task31-apache-deployment-55478b4d75-9b7mp\" (UID: \"b3c15d86-ec97-442c-b5b3-32e669de6e64\") " pod="default/task31-apache-deployment-55478b4d75-9b7mp"
May 08 07:30:31 minikube kubelet[1568]: I0508 07:30:31.052809    1568 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ln295\" (UniqueName: \"kubernetes.io/projected/c7f566f6-d6c5-4b3b-ae7b-efcfadf50931-kube-api-access-ln295\") pod \"task31-nginx-deployment-744d867495-nt4b8\" (UID: \"c7f566f6-d6c5-4b3b-ae7b-efcfadf50931\") " pod="default/task31-nginx-deployment-744d867495-nt4b8"
May 08 07:30:40 minikube kubelet[1568]: I0508 07:30:40.490126    1568 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/task31-nginx-deployment-744d867495-nt4b8" podStartSLOduration=2.202507626 podCreationTimestamp="2024-05-08 07:30:30 +0000 UTC" firstStartedPulling="2024-05-08 07:30:31.494933099 +0000 UTC m=+546.277240002" lastFinishedPulling="2024-05-08 07:30:39.782473787 +0000 UTC m=+554.564780689" observedRunningTime="2024-05-08 07:30:40.489551742 +0000 UTC m=+555.271859134" watchObservedRunningTime="2024-05-08 07:30:40.490048313 +0000 UTC m=+555.272356124"
May 08 07:30:59 minikube kubelet[1568]: I0508 07:30:59.779478    1568 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/task31-apache-deployment-55478b4d75-9b7mp" podStartSLOduration=4.69062773 podCreationTimestamp="2024-05-08 07:30:30 +0000 UTC" firstStartedPulling="2024-05-08 07:30:31.500387342 +0000 UTC m=+546.282694663" lastFinishedPulling="2024-05-08 07:30:56.589140022 +0000 UTC m=+571.371447903" observedRunningTime="2024-05-08 07:30:59.779255116 +0000 UTC m=+574.561562927" watchObservedRunningTime="2024-05-08 07:30:59.77938097 +0000 UTC m=+574.561688361"

* 
* ==> kubernetes-dashboard [744a9d3bbfbb] <==
* 2024/05/08 07:22:18 Starting overwatch
2024/05/08 07:22:18 Using namespace: kubernetes-dashboard
2024/05/08 07:22:18 Using in-cluster config to connect to apiserver
2024/05/08 07:22:18 Using secret token for csrf signing
2024/05/08 07:22:18 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/08 07:22:18 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/05/08 07:22:18 Successful initial request to the apiserver, version: v1.28.3
2024/05/08 07:22:18 Generating JWE encryption key
2024/05/08 07:22:18 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/05/08 07:22:18 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/05/08 07:22:18 Initializing JWE encryption key from synchronized object
2024/05/08 07:22:18 Creating in-cluster Sidecar client
2024/05/08 07:22:18 Successful request to sidecar
2024/05/08 07:22:18 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [ca6db11709b6] <==
* 2024/05/08 07:21:30 Using namespace: kubernetes-dashboard
2024/05/08 07:21:30 Using in-cluster config to connect to apiserver
2024/05/08 07:21:30 Using secret token for csrf signing
2024/05/08 07:21:30 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/05/08 07:21:30 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00075fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0002ce480)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [23b935cbf47d] <==
* I0508 07:22:14.729376       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0508 07:22:14.744402       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0508 07:22:14.745614       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0508 07:22:32.151345       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0508 07:22:32.151546       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"c6bc4dd5-02a2-46a6-8a10-c1d34ec06109", APIVersion:"v1", ResourceVersion:"28571", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e2dda7a8-1096-4ab4-83fb-502c66a2fbe5 became leader
I0508 07:22:32.151754       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e2dda7a8-1096-4ab4-83fb-502c66a2fbe5!
I0508 07:22:32.252903       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e2dda7a8-1096-4ab4-83fb-502c66a2fbe5!

* 
* ==> storage-provisioner [80365797c769] <==
* I0508 07:21:30.656587       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0508 07:22:00.676291       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

